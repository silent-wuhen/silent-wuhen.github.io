<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>python爬虫 | My Blog</title><meta name="author" content="HQH"><meta name="copyright" content="HQH"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="前言学习视频：尚硅谷 学习前最好是有一定的python基础，学习的效率会更加的好。 CTRL + &#x2F; 快速注释 1.前提知识1.序列化与反序列化1.概念通过文件操作，可以将字符串写入到一个本地文件。但是，如果是一个对象(例如列表、字典、元组等)，就无法直接写入到一个文件里，需要对这个对象进行序列化，然后才能写入到文件里。 设计一套协议，按照某种规则，把内存中的数据转换为字节序列，保存到文">
<meta property="og:type" content="article">
<meta property="og:title" content="python爬虫">
<meta property="og:url" content="http://example.com/2023/11/01/python%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="My Blog">
<meta property="og:description" content="前言学习视频：尚硅谷 学习前最好是有一定的python基础，学习的效率会更加的好。 CTRL + &#x2F; 快速注释 1.前提知识1.序列化与反序列化1.概念通过文件操作，可以将字符串写入到一个本地文件。但是，如果是一个对象(例如列表、字典、元组等)，就无法直接写入到一个文件里，需要对这个对象进行序列化，然后才能写入到文件里。 设计一套协议，按照某种规则，把内存中的数据转换为字节序列，保存到文">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/%E5%9B%BE%E7%89%87/014.jpg">
<meta property="article:published_time" content="2023-11-01T15:16:12.000Z">
<meta property="article:modified_time" content="2025-05-21T03:12:15.173Z">
<meta property="article:author" content="HQH">
<meta property="article:tag" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/%E5%9B%BE%E7%89%87/014.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/11/01/python%E7%88%AC%E8%99%AB/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'python爬虫',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-05-21 11:12:15'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://www.fomal.cc/static/css/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/touxiang.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/%E5%9B%BE%E7%89%87/014.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="My Blog"></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><!-- span=' '+_p('search.title')--></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">python爬虫</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-11-01T15:16:12.000Z" title="发表于 2023-11-01 23:16:12">2023-11-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-21T03:12:15.173Z" title="更新于 2025-05-21 11:12:15">2025-05-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/H04-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%8C%96%E6%8E%98/">H04_数据分析与挖掘</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">13.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>61分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="python爬虫"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>学习视频：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Db4y1m7Ho?p=49&vd_source=bd0b10b1a89eeb70aae017bf4b5233c3">尚硅谷</a></p>
<p>学习前最好是有一定的python基础，学习的效率会更加的好。</p>
<p>CTRL + &#x2F; 快速注释</p>
<h1 id="1-前提知识"><a href="#1-前提知识" class="headerlink" title="1.前提知识"></a>1.前提知识</h1><h2 id="1-序列化与反序列化"><a href="#1-序列化与反序列化" class="headerlink" title="1.序列化与反序列化"></a>1.序列化与反序列化</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h3><p>通过文件操作，可以将字符串写入到一个本地文件。但是，如果是一个对象(例如列表、字典、元组等)，就无法直接写入到一个文件里，需要对这个对象进行序列化，然后才能写入到文件里。</p>
<p>设计一套协议，按照某种规则，把内存中的数据转换为字节序列，保存到文件，这就是序列化，反之，从文件的字节序列恢复到内存中，就是反序列化。</p>
<ul>
<li>对象—&gt;字节序列 &#x3D;&#x3D;&#x3D; 序列化</li>
<li>字节序列—&gt;对象 &#x3D;&#x3D;&#x3D;反序列化</li>
</ul>
<p>Python中提供了JSON这个模块用来实现数据的序列化和反序列化。</p>
<h3 id="2-序列化的实现"><a href="#2-序列化的实现" class="headerlink" title="2.序列化的实现"></a>2.序列化的实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对象 ---&gt; 字节序列 == 序列化</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment">#序列化有2种:dumps和dump</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#第一种dumps</span></span><br><span class="line">fp = <span class="built_in">open</span>(<span class="string">&#x27;test.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">name_list = [<span class="string">&#x27;zs&#x27;</span>,<span class="string">&#x27;ls&#x27;</span>]</span><br><span class="line">names = json.dumps(name_list)<span class="comment">#序列化</span></span><br><span class="line">fp.write(names)</span><br><span class="line">fp.close()</span><br><span class="line"><span class="built_in">print</span>(names,<span class="built_in">type</span>(names)) <span class="comment"># 字符串类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#第二种dump</span></span><br><span class="line"><span class="comment">#在将对象转换成字符串的同时，指定一个文件的对象，然后把转换后的字符串写入到这个文件里</span></span><br><span class="line">fp = <span class="built_in">open</span>(<span class="string">&#x27;text.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">name_list = [<span class="string">&#x27;zs&#x27;</span>,<span class="string">&#x27;ls&#x27;</span>]</span><br><span class="line">json.dump(name_list,fp)<span class="comment">#序列化</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">相当于将</span></span><br><span class="line"><span class="string">names = json.dumps(name_list)和</span></span><br><span class="line"><span class="string">fp.write(names)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">fp.close()</span><br></pre></td></tr></table></figure>



<h3 id="3-反序列化的实现"><a href="#3-反序列化的实现" class="headerlink" title="3.反序列化的实现"></a>3.反序列化的实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 字节序列 变为 对象 == 反序列化</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment">#反序列化:loads,load</span></span><br><span class="line"><span class="comment">#将json的字符串变成一个python对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#第一种loads</span></span><br><span class="line">fp = <span class="built_in">open</span>(<span class="string">&#x27;test.txt&#x27;</span>,<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">content = fp.read()</span><br><span class="line">result = json.loads(content)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(result))</span><br><span class="line">fp.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">#第二种load</span></span><br><span class="line">fp = <span class="built_in">open</span>(<span class="string">&#x27;text.txt&#x27;</span>,<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">result = json.load(fp)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(result))</span><br><span class="line">fp.close()</span><br></pre></td></tr></table></figure>



<h2 id="2-异常"><a href="#2-异常" class="headerlink" title="2.异常"></a>2.异常</h2><h3 id="1-概念-1"><a href="#1-概念-1" class="headerlink" title="1.概念"></a>1.概念</h3><p>程序在运行过程中，由于编码不规范，或者其他原因一些客观原因，导致程序无法继续运行，此时，程序就会出现异常。如果我们不对异常进行处理，程序可能会由于异常直接中断掉。为了保证程序的健壮性，我们在程序设计里提出了异常处理这个概念。</p>
<h3 id="2-try…except语句"><a href="#2-try…except语句" class="headerlink" title="2.try…except语句"></a>2.try…except语句</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语法：</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">	可能会出现异常的代码块</span><br><span class="line"><span class="keyword">except</span> 异常的类型:</span><br><span class="line">	出现异常以后的处理语句（友好的提示）</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 实例：</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    f = <span class="built_in">open</span>(<span class="string">&#x27;test.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.read())</span><br><span class="line"><span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;文件没有找到,请检查文件名称是否正确&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="3-网页页面结构的介绍"><a href="#3-网页页面结构的介绍" class="headerlink" title="3.网页页面结构的介绍"></a>3.网页页面结构的介绍</h2><h3 id="1-html文件"><a href="#1-html文件" class="headerlink" title="1.html文件"></a>1.html文件</h3><p>新建一个.html文件，初始的主体为：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="2-html的标签介绍"><a href="#2-html的标签介绍" class="headerlink" title="2.html的标签介绍"></a>2.html的标签介绍</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">    table    表格</span></span><br><span class="line"><span class="comment">    tr       行</span></span><br><span class="line"><span class="comment">    td       列</span></span><br><span class="line"><span class="comment">    --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">table</span> <span class="attr">width</span>=<span class="string">&quot;200px&quot;</span> <span class="attr">height</span>=<span class="string">&quot;200px&quot;</span> <span class="attr">border</span>=<span class="string">&quot;1px&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                    姓名</span><br><span class="line">                <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                    年龄</span><br><span class="line">                <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                    性别</span><br><span class="line">                <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                    你</span><br><span class="line">                <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                    好</span><br><span class="line">                <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                    啊</span><br><span class="line">                <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ul li  无序列表（爬虫使用的场景非常的多）--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span>&gt;</span>我最帅<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- ol li  有序列表--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">ol</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span>&gt;</span>洗衣<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span>&gt;</span>吃饭<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">ol</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 超链接--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;https://silent-wuhen.github.io/&quot;</span>&gt;</span>MyBlog<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h2 id="4-URL（统一资源定位符）组成"><a href="#4-URL（统一资源定位符）组成" class="headerlink" title="4.URL（统一资源定位符）组成"></a>4.URL（统一资源定位符）组成</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#url的组成：协议(http/https)，主机(www.baidu.com)，端口号(80/443)，路径，参数，锚点</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#https://www.baidu.com/s?wd=%E5%91%A8%E6%9D%B0%E4%BC%A6</span></span><br><span class="line"><span class="comment">#协议：https    主机：www.baidu.com   端口号：443   路径：s  参数：？以后  锚点：#</span></span><br></pre></td></tr></table></figure>



<h2 id="5-什么是互联网爬虫"><a href="#5-什么是互联网爬虫" class="headerlink" title="5.什么是互联网爬虫"></a>5.什么是互联网爬虫</h2><ul>
<li>解释1：通过一个程序，根据Url(<a target="_blank" rel="noopener" href="http://www.taobao.com)进行爬取网页,获取有用信息/">http://www.taobao.com)进行爬取网页，获取有用信息</a></li>
<li>解释2：使用程序模拟浏览器，去向服务器发送请求，获取响应信息</li>
</ul>
<h2 id="6-爬虫核心"><a href="#6-爬虫核心" class="headerlink" title="6.爬虫核心"></a>6.爬虫核心</h2><ol>
<li>爬取网页：爬取整个网页，包含了网页中所有的内容</li>
<li>解析数据：将网页中得到的数据进行解析</li>
<li>难点：爬虫与反爬虫之间的博弈</li>
</ol>
<h2 id="7-爬虫的用途"><a href="#7-爬虫的用途" class="headerlink" title="7.爬虫的用途"></a>7.爬虫的用途</h2><ol>
<li>数据分析&#x2F;人工数据集</li>
<li>社交软件冷启动</li>
<li>竞争对手的监控</li>
<li>舆情监控</li>
</ol>
<h2 id="8-爬虫的分类"><a href="#8-爬虫的分类" class="headerlink" title="8.爬虫的分类"></a>8.爬虫的分类</h2><h3 id="1-通用爬虫-不是我们学习的"><a href="#1-通用爬虫-不是我们学习的" class="headerlink" title="1.通用爬虫(不是我们学习的)"></a>1.通用爬虫(不是我们学习的)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">实例:</span><br><span class="line">	百度、<span class="number">360</span>、google、sougou等搜索引擎‐‐‐伯乐在线</span><br><span class="line"></span><br><span class="line">功能:</span><br><span class="line">	访问网页‐&gt;抓取数据‐&gt;数据存储‐&gt;数据处理‐&gt;提供检索服务</span><br><span class="line">    </span><br><span class="line">robots协议:</span><br><span class="line">	一个约定俗成的协议，添加robots.txt文件，来说明本网站哪些内容不可以被抓取，起不到限制作用</span><br><span class="line">	自己写的爬虫无需遵守</span><br><span class="line">    </span><br><span class="line">网站排名(SEO):</span><br><span class="line">	<span class="number">1.</span> 根据pagerank算法值进行排名（参考个网站流量、点击率等指标）</span><br><span class="line">	<span class="number">2.</span> 百度竞价排名</span><br><span class="line">    </span><br><span class="line">缺点:</span><br><span class="line">	<span class="number">1.</span> 抓取的数据大多是无用的</span><br><span class="line">	<span class="number">2.</span>不能根据用户的需求来精准获取数据</span><br></pre></td></tr></table></figure>



<h3 id="2-聚焦爬虫"><a href="#2-聚焦爬虫" class="headerlink" title="2.聚焦爬虫"></a>2.聚焦爬虫</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">功能：</span><br><span class="line">	根据需求，实现爬虫程序，抓取需要的数据</span><br><span class="line">设计思路：</span><br><span class="line"><span class="number">1.</span>确定要爬取的url</span><br><span class="line">	​如何获取url</span><br><span class="line"><span class="number">2.</span>模拟浏览器通过http协议访问url,获取服务器返回的html代码</span><br><span class="line">	​如何访问</span><br><span class="line"><span class="number">3.</span>解析html字符串（根据一定的规则提取需要的数据）</span><br><span class="line">	​如何解析</span><br></pre></td></tr></table></figure>



<h2 id="9-反爬虫手段？"><a href="#9-反爬虫手段？" class="headerlink" title="9.反爬虫手段？"></a>9.反爬虫手段？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>User‐Agent：</span><br><span class="line">    ​User Agent中文名为用户代理，简称 UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本、浏览器渲染引擎、浏览器语言、浏览器插件等。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span>代理IP</span><br><span class="line">    ​西次代理</span><br><span class="line">    ​快代理</span><br><span class="line">    ​什么是高匿名、匿名和透明代理？它们有什么区别？</span><br><span class="line">        ​<span class="number">1.</span>使用透明代理，对方服务器可以知道你使用了代理，并且也知道你的真实IP。</span><br><span class="line">        ​<span class="number">2.</span>使用匿名代理，对方服务器可以知道你使用了代理，但不知道你的真实IP。</span><br><span class="line">        ​<span class="number">3.</span>使用高匿名代理，对方服务器不知道你使用了代理，更不知道你的真实IP。</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span>验证码访问</span><br><span class="line">    ​打码平台</span><br><span class="line">    ​云打码平台</span><br><span class="line">    ​超级🦅</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span>动态加载网页 </span><br><span class="line">    ​网站返回的是js数据 并不是网页的真实数据</span><br><span class="line">    ​selenium驱动真实的浏览器发送请求</span><br><span class="line"></span><br><span class="line"><span class="number">5.</span>数据加密</span><br><span class="line">    ​分析js代码</span><br></pre></td></tr></table></figure>



<h1 id="2-Urllib"><a href="#2-Urllib" class="headerlink" title="2.Urllib"></a>2.Urllib</h1><h2 id="1-urllib库使用"><a href="#1-urllib库使用" class="headerlink" title="1.urllib库使用"></a>1.urllib库使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlopen() 模拟浏览器向服务器发送请求</span><br><span class="line"></span><br><span class="line">response 服务器返回的数据</span><br><span class="line">    response的数据类型是HttpResponse</span><br><span class="line">    字节‐‐&gt;字符串</span><br><span class="line">    	解码decode</span><br><span class="line">    字符串‐‐&gt;字节</span><br><span class="line">    	编码encode</span><br><span class="line">    read() 字节形式读取二进制 扩展：rede(<span class="number">5</span>)返回前几个字节</span><br><span class="line">    readline() 读取一行</span><br><span class="line">    readlines() 一行一行读取 直至结束</span><br><span class="line">    getcode() 获取状态码</span><br><span class="line">    geturl() 获取url</span><br><span class="line">    getheaders() 获取headers</span><br><span class="line">    </span><br><span class="line">urllib.request.urlretrieve()</span><br><span class="line">    请求网页</span><br><span class="line">    请求图片</span><br><span class="line">    请求视频</span><br></pre></td></tr></table></figure>



<h3 id="1-urllib的基本使用"><a href="#1-urllib的基本使用" class="headerlink" title="1.urllib的基本使用"></a>1.urllib的基本使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用urllib来获取百度首页的源码</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="comment"># （1）定义一个url:就是要访问的地址</span></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># （2）模拟浏览器向服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(url) <span class="comment"># response响应：返回的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># （3）获取响应中的页面源码  content 内容</span></span><br><span class="line">    <span class="comment"># read 返回的是字节形式的二进制数据</span></span><br><span class="line">    <span class="comment"># 二进制--&gt;字符串称为解码  使用的方法是decode(&#x27;解码的格式&#x27;)</span></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># （4）打印数据</span></span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>



<h3 id="2-Urllib的1个类型和6个方法"><a href="#2-Urllib的1个类型和6个方法" class="headerlink" title="2.Urllib的1个类型和6个方法"></a>2.Urllib的1个类型和6个方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个类型和六个方法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#一个类型：HTTPResponse</span></span><br><span class="line"><span class="comment"># response是HTTPResponse数据类型的</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;response的数据类型：&#x27;</span>,<span class="built_in">type</span>(response))</span><br><span class="line"></span><br><span class="line"><span class="comment">#六个方法：read() readline() readlines() getcode() geturl() getheaders()</span></span><br><span class="line"><span class="comment"># read()按照一个字节一个字节的读</span></span><br><span class="line">content = response.read()</span><br><span class="line"><span class="comment">#print(content)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.返回多少个字节</span></span><br><span class="line">content = response.read(<span class="number">5</span>)</span><br><span class="line"><span class="comment">#print(content)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.读取一行（只能读取一行）</span></span><br><span class="line">content = response.readline()</span><br><span class="line"><span class="comment">#print(content)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.一行一行的读，读取所有行，要编码使用decode(&#x27;utf-8&#x27;)</span></span><br><span class="line">content = response.readlines()</span><br><span class="line"><span class="comment">#print(content)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.返回状态码 如果是200：证明逻辑正确（判断代码是否有问题）</span></span><br><span class="line"><span class="built_in">print</span>(response.getcode())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.返回的是url的地址</span></span><br><span class="line"><span class="built_in">print</span>(response.geturl())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.获取的是状态信息</span></span><br><span class="line"><span class="built_in">print</span>(response.getheaders())</span><br></pre></td></tr></table></figure>



<h3 id="3-Urllib的下载"><a href="#3-Urllib的下载" class="headerlink" title="3.Urllib的下载"></a>3.Urllib的下载</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urlretrieve(url,filename):url 下载路径，filename 文件的名字</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载网页</span></span><br><span class="line">url_page = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line">urllib.request.urlretrieve(url_page,<span class="string">&#x27;baidu.html&#x27;</span>)</span><br><span class="line"><span class="comment"># urllib.request.urlretrieve(url=url_page,filename=&#x27;baidu.html&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载图片</span></span><br><span class="line">url_img = <span class="string">&#x27;https://img2.baidu.com/it/u=2064031713,2731059264&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=500&amp;h=500&#x27;</span></span><br><span class="line"></span><br><span class="line">urllib.request.urlretrieve(url_img,<span class="string">&#x27;平泽唯.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载视频</span></span><br><span class="line">url_video = <span class="string">&#x27;https://vd7.bdstatic.com/mda-pdjhxmd8mxyvgexz/cae_h264/1682010451294745439/mda-pdjhxmd8mxyvgexz.mp4?v_from_s=hkapp-haokan-hbf&amp;auth_key=1682077284-0-0-fbb39f9dcd76b70a58e232aa536a6a7b&amp;bcevod_channel=searchbox_feed&amp;pd=1&amp;cd=0&amp;pt=3&amp;logid=0684217152&amp;vid=9752895408415982948&amp;abtest=109432_2-109133_1&amp;klogid=0684217152&amp;sdk_xcdn=1&#x27;</span></span><br><span class="line"></span><br><span class="line">urllib.request.urlretrieve(url_video,<span class="string">&#x27;测试视频.mp4&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="2-请求对象的定制"><a href="#2-请求对象的定制" class="headerlink" title="2.请求对象的定制"></a>2.请求对象的定制</h2><h3 id="1-UA介绍："><a href="#1-UA介绍：" class="headerlink" title="1.UA介绍："></a>1.UA介绍：</h3><p>User Agent中文名为<strong>用户代理</strong>，简称 UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本。浏览器内核、浏览器渲染引擎、浏览器语言、浏览器插件等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">语法：</span><br><span class="line">request = urllib.request.Request()</span><br></pre></td></tr></table></figure>



<h3 id="2-UA反爬"><a href="#2-UA反爬" class="headerlink" title="2.UA反爬"></a>2.UA反爬</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第一个反爬</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#url的组成：协议(http/https)，主机(www.baidu.com)，端口号(80/443)，路径，参数，锚点</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#https://www.baidu.com/s?wd=%E5%91%A8%E6%9D%B0%E4%BC%A6</span></span><br><span class="line"><span class="comment">#协议：https    主机：www.baidu.com   端口号：443   路径：s  参数：？后面  锚点：#</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">端口号</span></span><br><span class="line"><span class="string">http    80</span></span><br><span class="line"><span class="string">https   443</span></span><br><span class="line"><span class="string">mysql   3306</span></span><br><span class="line"><span class="string">oracle  1521</span></span><br><span class="line"><span class="string">redis   6379</span></span><br><span class="line"><span class="string">mongodb 27017</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 反爬</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.48&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#请求对象的定制</span></span><br><span class="line"><span class="comment">#注：因为参数顺序为url,data,headers，不能直接写url和headers</span></span><br><span class="line">request = urllib.request.Request(url = url,headers = headers)</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>





<h3 id="3-扩展：编码"><a href="#3-扩展：编码" class="headerlink" title="3.扩展：编码"></a>3.扩展：编码</h3><ol>
<li>ASCII编码：127个字符被编码到计算机里，也就是大小写英文字母、数字和一些符号。</li>
<li>GB2312编码：用来把中文编进去。</li>
<li>Unicode：最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。现代操作系统和大多数编程语言都直接支持Unicode。</li>
</ol>
<h2 id="3-编-解码"><a href="#3-编-解码" class="headerlink" title="3.编&#x2F;解码"></a>3.编&#x2F;解码</h2><h3 id="1-get请求方式：urllib-parse-quote（）"><a href="#1-get请求方式：urllib-parse-quote（）" class="headerlink" title="1.get请求方式：urllib.parse.quote（）"></a>1.get请求方式：urllib.parse.quote（）</h3><p>将汉字变成Unicode编码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语法：</span></span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line">name = urllib.parse.quote(<span class="string">&#x27;周杰伦&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>实际例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">需求</span></span><br><span class="line"><span class="string">获取 https://www.baidu.com/s?wd=周杰伦   的网页源码</span></span><br><span class="line"><span class="string">https://www.baidu.com/s?wd=%E5%91%A8%E6%9D%B0%E4%BC%A6</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="comment">#将周杰伦转换为Unicode编码格式</span></span><br><span class="line">name = urllib.parse.quote(<span class="string">&#x27;周杰伦&#x27;</span>)</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com/s?wd=&#x27;</span></span><br><span class="line">url = url + name</span><br><span class="line"></span><br><span class="line"><span class="comment">#解决反爬的手段</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.48&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 请求对象的定制</span></span><br><span class="line">request = urllib.request.Request(url = url,headers = headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取响应的内容</span></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印数据</span></span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>



<h3 id="2-get请求方式：urllib-parse-urlencode（）"><a href="#2-get请求方式：urllib-parse-urlencode（）" class="headerlink" title="2.get请求方式：urllib.parse.urlencode（）"></a>2.get请求方式：urllib.parse.urlencode（）</h3><ul>
<li>作用：将字典中多个中文转换成Unicode编码，并且键值之间使用“&#x3D;”连接，键值对之外使用&amp;连接。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">语法格式：</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">import urllib.parse</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">data = &#123;</span></span><br><span class="line"><span class="string">    &#x27;wd&#x27; : &#x27;周杰伦&#x27;,</span></span><br><span class="line"><span class="string">    &#x27;sex&#x27; : &#x27;男&#x27;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">a = urllib.parse.urlencode(data) # 将各个键值对用&amp;连接起来</span></span><br><span class="line"><span class="string">print(a)</span></span><br><span class="line"><span class="string">输出结果为：</span></span><br><span class="line"><span class="string">wd=%E5%91%A8%E6%9D%B0%E4%BC%A6&amp;sex=%E7%94%B7</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>实际例子：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urlencode应用场景：多个中文参数</span></span><br><span class="line"><span class="comment"># https://www.baidu.com/s?wd=周杰伦&amp;sex=男</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;wd&#x27;</span> : <span class="string">&#x27;周杰伦&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sex&#x27;</span> : <span class="string">&#x27;男&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">data = urllib.parse.urlencode(data)</span><br><span class="line"></span><br><span class="line">base_url = <span class="string">&#x27;https://www.baidu.com/s?&#x27;</span></span><br><span class="line">url = base_url + data</span><br><span class="line"><span class="comment"># print(url)</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.48&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(url = url,headers = headers)</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>



<h3 id="3-post请求方式"><a href="#3-post请求方式" class="headerlink" title="3.post请求方式"></a>3.post请求方式</h3><p>步骤：</p>
<ol>
<li>找post 请求的接口</li>
<li>怎么执行post请求（请求参数进行编码。编码之后 必须调用encode方法。）</li>
<li>传参数</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># post请求</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://fanyi.baidu.com/sug&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.48&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;kw&#x27;</span>:<span class="string">&#x27;spider&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># post请求的参数 必须要进行编码encode(&#x27;utf-8&#x27;)</span></span><br><span class="line">data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># post请求的参数是不会拼接在url的后面，而是需放在请求对象定制的参数中</span></span><br><span class="line">request = urllib.request.Request(url = url,data = data,headers = headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取响应的数据</span></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)<span class="comment">#str数据类型</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(content)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">post请求的参数 必须要进行编码  data = urllib.parse.urlencode(data).encode(&#x27;utf-8&#x27;)</span></span><br><span class="line"><span class="string">参数是放在请求对象的定制方法中  request = urllib.request.Request(url = url,data = data,headers = headers)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#字符串--&gt;json对象</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(content))</span><br><span class="line">obj = json.loads(content)</span><br><span class="line"><span class="built_in">print</span>(obj)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(obj))</span><br></pre></td></tr></table></figure>



<h3 id="4-总结：post和get区别？"><a href="#4-总结：post和get区别？" class="headerlink" title="4.总结：post和get区别？"></a>4.总结：post和get区别？</h3><ol>
<li>get请求方式的参数必须编码，参数是拼接到url后面，编码之后不需要调用encode方法</li>
<li>post请求方式的参数必须编码，参数是放在请求对象定制的方法中，编码之后需要调用encode方法</li>
</ol>
<h3 id="5-百度翻译之详细翻译"><a href="#5-百度翻译之详细翻译" class="headerlink" title="5.百度翻译之详细翻译"></a>5.百度翻译之详细翻译</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://fanyi.baidu.com/v2transapi?from=en&amp;to=zh&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;*/*&#x27;</span>,</span><br><span class="line">    <span class="comment"># &#x27;Accept-Encoding&#x27;: &#x27;gzip, deflate, br&#x27;, # 这是编码格式，没有UTF-8，必须注释掉</span></span><br><span class="line">    <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;zh-CN,zh;q=0.9&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Acs-Token&#x27;</span>: <span class="string">&#x27;1705576134035_1705654546778_pgSxk6qRvhC4+gm1SagDeIwwKcAGC365pA/wc0YZZyri/YI6h/8LSa5pIQ1G2rdtWlVoV9ibHEqExlMbbtd8V/jAVJH2e8ZpisX3ddWOOmP/WnrBLhXD38MHyAwl5/XWsJffX33TRC4K9Vsz8zQOW4NPyG7kLTJZGPDTCv1iaYQWgr4WJbih1fOWAXb+MgTa15EDPWLWTw8+lVEQTvK8Oans77TkOwbjvcAE1l3a6YawdzjJI0NbYG+Xk9PnDmy3JAg+5r4cxuBhfc6K0mY+2mDbFQ3iQyEnSszmLxWvAo08dvBVc7m8I18SyiGbHzNdOPAu+Qep37iPKOklgqrShhz9+jQfQaeJ54E8N/HrV/9chA0aJqkqPcQyz1z8nXXn57TEFVZV8dhti5xv7lw0SJetoL8BGfInUYMtxOovg7VDvW4BI192MW2ocJaIu1NoDhrAtKSJb4I58Q3k0dxn/Pn40xyb35uXK1e5MUykZ3ThxdahuJLHaVch95PuMyG0GuWV1hcQc9Lq3jRLc/jrQw==&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Connection&#x27;</span>: <span class="string">&#x27;keep-alive&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Content-Length&#x27;</span>: <span class="string">&#x27;149&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Content-Type&#x27;</span>: <span class="string">&#x27;application/x-www-form-urlencoded; charset=UTF-8&#x27;</span>,</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 其中去决定性作用的是Cookie，其他的都是可以注释掉的</span></span><br><span class="line">    <span class="string">&#x27;Cookie&#x27;</span>: <span class="string">&#x27;BIDUPSID=51D9D30414A922DDC3F952B560F37FC2; PSTM=1652673230; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; BAIDUID=2552FC12B56D5C63A22F4860676B0A88:FG=1; BDUSS=25aNHBtd2VxOUgyNnpkUkd-TFJSejZJMWlFYU10ZjlMMEt6LTBldnlPcEJqb3BsSVFBQUFBJCQAAAAAAAAAAAEAAACDPaTeAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEEBY2VBAWNlb; BDUSS_BFESS=25aNHBtd2VxOUgyNnpkUkd-TFJSejZJMWlFYU10ZjlMMEt6LTBldnlPcEJqb3BsSVFBQUFBJCQAAAAAAAAAAAEAAACDPaTeAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEEBY2VBAWNlb; H_WISE_SIDS=39999_40016_40041; H_PS_PSSID=39999_40016_40041; H_WISE_SIDS_BFESS=39999_40016_40041; BAIDUID_BFESS=2552FC12B56D5C63A22F4860676B0A88:FG=1; BDRCVFR[A7hGrXFW48R]=K7PavkeTFPTUAN8ULuEQhPEUi4WU6; PSINO=2; BA_HECTOR=24a40la4a50l052h202h8k2gdtqjju1iqkdv61s; ZFY=JxHMjLMptFSB0Fx6rLKcRUU:AIGXbEGxWGnIfnIlrcrY:C; BDORZ=FFFB88E999055A3F8A630C64834BD6D0; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1705150908,1705654260; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1705654540; ab_sr=1.0.1_MjBlNjBiNGE0YjVmNGI4NTRjYzBjYzUyYzU2Mzg4ZTZiMzY4Zjc3ODU1MzgwMjEyZGUwYjhhNDliYzZlYzM5M2U2YzZiNzQyOGZhNjc3OTNhY2JmM2ExNjg5YTdlOWY5Y2ZmODUwODViMTJmNjc3NGYwNzA0MmUwZTVjMzZhNmNiOTI4ZTBmNTQzZTgyYjgxYWRjNmQxN2Q0NmFiYTk4ZmZjOTdlZTYyMmJkMzY4ZTAwYTJiZjBhOGFkNWMxMTJi&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Host&#x27;</span>: <span class="string">&#x27;fanyi.baidu.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Origin&#x27;</span>: <span class="string">&#x27;https://fanyi.baidu.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;https://fanyi.baidu.com/translate?query=&amp;keyfrom=baidu&amp;smartresult=dict&amp;lang=auto2zh&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua&#x27;</span>: <span class="string">&#x27;&quot;Chromium&quot;;v=&quot;9&quot;, &quot;Not?A_Brand&quot;;v=&quot;8&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-mobile&#x27;</span>: <span class="string">&#x27;?0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-platform&#x27;</span>: <span class="string">&#x27;&quot;Windows&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Dest&#x27;</span>: <span class="string">&#x27;empty&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Mode&#x27;</span>: <span class="string">&#x27;cors&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Site&#x27;</span>: <span class="string">&#x27;same-origin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 SLBrowser/9.0.0.10191 SLBChan/105&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;X-Requested-With&#x27;</span>: <span class="string">&#x27;XMLHttpRequest&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;from&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;to&#x27;</span>: <span class="string">&#x27;zh&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;query&#x27;</span>: <span class="string">&#x27;love&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;transtype&#x27;</span>: <span class="string">&#x27;enter&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;simple_means_flag&#x27;</span>: <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sign&#x27;</span>: <span class="string">&#x27;198772.518981&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;token&#x27;</span>: <span class="string">&#x27;7483d683cdef5a92727320d35cbcdbf3&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;domain&#x27;</span>: <span class="string">&#x27;common&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;ts&#x27;</span>: <span class="string">&#x27;1705654546761&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># post请求的参数是不会拼接在url的后面，而是需放在请求对象定制的参数中</span></span><br><span class="line">request = urllib.request.Request(url = url,data = data,headers = headers)</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)<span class="comment">#str数据类型</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(content)</span><br><span class="line"></span><br><span class="line"><span class="comment">#字符串--&gt;json对象</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">obj = json.loads(content)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(obj)</span><br></pre></td></tr></table></figure>



<h2 id="4-ajax的get请求"><a href="#4-ajax的get请求" class="headerlink" title="4.ajax的get请求"></a>4.ajax的get请求</h2><ul>
<li>案例1：豆瓣电影第一页</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get请求</span></span><br><span class="line"><span class="comment"># 获取第一页数据并且保存</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://movie.douban.com/j/chart/top_list?type=6&amp;interval_id=100%3A90&amp;action=&amp;start=0&amp;limit=20&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.48&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(url = url,headers = headers)</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#print(content)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># open默认使用的是gbk的编码</span></span><br><span class="line"><span class="comment"># fp = open(&quot;douban.json&quot;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;)</span></span><br><span class="line"><span class="comment"># fp.write(content) # ctrl+alt+l 调整格式</span></span><br><span class="line"><span class="comment"># fp.close</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;豆瓣.json&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(content)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>案例2：豆瓣电影前10页</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">https://movie.douban.com/j/chart/top_list?type=5&amp;interval_id=100%3A90&amp;action=&amp;</span></span><br><span class="line"><span class="string">start=0&amp;limit=20</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://movie.douban.com/j/chart/top_list?type=5&amp;interval_id=100%3A90&amp;action=&amp;</span></span><br><span class="line"><span class="string">start=20&amp;limit=20</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://movie.douban.com/j/chart/top_list?type=5&amp;interval_id=100%3A90&amp;action=&amp;</span></span><br><span class="line"><span class="string">start=40&amp;limit=20</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">规律：</span></span><br><span class="line"><span class="string">start : (page - 1) * 20</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">步骤：</span></span><br><span class="line"><span class="string">1.请求对象的定制</span></span><br><span class="line"><span class="string">2.获取相应的数据</span></span><br><span class="line"><span class="string">3.数据下载</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_request</span>(<span class="params">page</span>):</span><br><span class="line">    base_url = <span class="string">&#x27;https://movie.douban.com/j/chart/top_list?type=5&amp;interval_id=100%3A90&amp;action=&amp;&#x27;</span></span><br><span class="line"></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&#x27;start&#x27;</span> : (page - <span class="number">1</span>) * <span class="number">20</span>,</span><br><span class="line">        <span class="string">&#x27;limit&#x27;</span> : <span class="number">20</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    data = urllib.parse.urlencode(data)</span><br><span class="line">    url = base_url + data</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.48&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    request = urllib.request.Request(url = url, headers = headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">request</span>):</span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_load</span>(<span class="params">page,content</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;豆瓣&quot;</span> + <span class="built_in">str</span>(page) + <span class="string">&quot;.json&quot;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(content)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 程序的入口</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;请输入起始的页码：&quot;</span>))</span><br><span class="line">    end = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;请输入结束的页码：&quot;</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start,end +<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 每页都需要进行请求对象的定制</span></span><br><span class="line">        request = create_request(page)</span><br><span class="line">        <span class="comment"># 获取的响应的数据</span></span><br><span class="line">        content = get_content(request)</span><br><span class="line">        <span class="comment"># 下载</span></span><br><span class="line">        down_load(page,content)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="5-ajax的post请求"><a href="#5-ajax的post请求" class="headerlink" title="5.ajax的post请求"></a>5.ajax的post请求</h2><ul>
<li>案例：爬取肯德基官网</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">page 1</span></span><br><span class="line"><span class="string">http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname</span></span><br><span class="line"><span class="string">post请求</span></span><br><span class="line"><span class="string">cname: 北京</span></span><br><span class="line"><span class="string">pid:</span></span><br><span class="line"><span class="string">pageIndex: 1</span></span><br><span class="line"><span class="string">pageSize: 10</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">page 2</span></span><br><span class="line"><span class="string">http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname</span></span><br><span class="line"><span class="string">post请求</span></span><br><span class="line"><span class="string">cname: 北京</span></span><br><span class="line"><span class="string">pid:</span></span><br><span class="line"><span class="string">pageIndex: 2</span></span><br><span class="line"><span class="string">pageSize: 10</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">规律：pageIndex（第几页就是几）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_request</span>():</span><br><span class="line">    base_url = <span class="string">&#x27;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname&#x27;</span></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&#x27;post&#x27;</span>:<span class="string">&#x27;请求&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;cname&#x27;</span>:<span class="string">&#x27;北京&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pid&#x27;</span>:<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pageIndex&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">&#x27;pageSize&#x27;</span>: <span class="number">10</span></span><br><span class="line">    &#125;</span><br><span class="line">    data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.48&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    request = urllib.request.Request(url=base_url,data=data,headers=headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">request</span>):</span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_load</span>(<span class="params">page,content</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;肯德基_&#x27;</span>+<span class="built_in">str</span>(page)+<span class="string">&#x27;.json&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;请输入起始页码：&quot;</span>))</span><br><span class="line">    end_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;请输入终止页码：&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start_page,end_page + <span class="number">1</span>):</span><br><span class="line">        request = create_request()</span><br><span class="line">        content = get_content(request)</span><br><span class="line">        down_load(page,content)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="6-URLError-HTTPError"><a href="#6-URLError-HTTPError" class="headerlink" title="6.URLError\HTTPError"></a>6.URLError\HTTPError</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">简介:</span><br><span class="line"><span class="number">1.</span>HTTPError类是URLError类的子类</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span>导入的包urllib.error.HTTPError urllib.error.URLError</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span>http错误：http错误是针对浏览器无法连接到服务器而增加出来的错误提示。引导并告诉浏览者该页是哪里出了问题。</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span>通过urllib发送请求的时候，有可能会发送失败，这个时候如果想让你的代码更加的健壮，可以通过<span class="keyword">try</span>‐<span class="keyword">except</span>进行捕获异常，异常有两类，URLError\HTTPError</span><br></pre></td></tr></table></figure>



<ul>
<li>案例：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"></span><br><span class="line"><span class="comment"># import</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># url = &#x27;https://blog.csdn.net/Purpleendurer/article/details/135156750&#x27; # 正确的url</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># url = &#x27;https://blog.csdn.net/Purpleendurer/article/details/13515675011&#x27; # httperror</span></span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.hqhwwww.com&#x27;</span>  <span class="comment"># urlerror</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.48&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line"></span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(content)</span><br><span class="line"><span class="keyword">except</span> urllib.error.HTTPError:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;系统正在升级&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;嗯，还在升级&quot;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="7-cookie登录"><a href="#7-cookie登录" class="headerlink" title="7.cookie登录"></a>7.cookie登录</h2><ul>
<li>案例：微博登录</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">适用的场景：数据采集的时候，需要绕过登录，然后进入到某个页面</span></span><br><span class="line"><span class="string">个人信息页面是utf-8,但是还是保编码错误，因为没有登录信息页面，而是跳转到了登录页面，登录页面不是UTF-8，所以报错</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://m.weibo.cn/users/2906598961?set=1&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;accept-language&#x27;</span>: <span class="string">&#x27;zh-CN,zh;q=0.9&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;cache-control&#x27;</span>: <span class="string">&#x27;max-age=0&#x27;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="comment"># cookie中携带着登录信息，如果有登录之后的cookie 那么我们可以携带着cookie进入任何页面</span></span><br><span class="line">    <span class="string">&#x27;cookie&#x27;</span>: <span class="string">&#x27;_T_WM=f2795e7918b16c688965f9651943adf8; XSRF-TOKEN=f01dfc; MLOGIN=1; SUB=_2A25Ir4huDeRhGeRH61QU-SbFzT2IHXVrxIWmrDV6PUJbktAGLU_EkW1NTfzTjDYP_oJIck5f95UkomM2UsPW2zlt; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9W5FThBRqddjayf_HAMY69ZD5JpX5KzhUgL.Foz4ehqf1Kn4So22dJLoIX5LxK-L12qLB--LxK-LB.BL1KeLxKqLBoeL1K-LxKqL1-eLB-qLxK-L12qL1KnLxK-L12qL1KnLxK-L12qL1KnE; SSOLoginState=1705769022; ALF=1708361022&#x27;</span>,</span><br><span class="line">    <span class="comment"># referer（做防盗链） 判断当前路径是不是由上一个路径进来的，一般情况下是做图片的防盗链</span></span><br><span class="line">    <span class="string">&#x27;referer&#x27;</span>: <span class="string">&#x27;https://m.weibo.cn/profile/2906598961&#x27;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;sec-ch-ua&#x27;</span>: <span class="string">&#x27;&quot;Chromium&quot;;v=&quot;9&quot;, &quot;Not?A_Brand&quot;;v=&quot;8&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-mobile&#x27;</span>: <span class="string">&#x27;?0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-platform&#x27;</span>: <span class="string">&#x27;&quot;Windows&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-dest&#x27;</span>: <span class="string">&#x27;empty&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-mode&#x27;</span>: <span class="string">&#x27;navigate&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-site&#x27;</span>: <span class="string">&#x27;same-origin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;upgrade-insecure-requests&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 SLBrowser/9.0.0.10191 SLBChan/105&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;weibo.html&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(content)</span><br></pre></td></tr></table></figure>



<h2 id="8-Handler处理器"><a href="#8-Handler处理器" class="headerlink" title="8.Handler处理器"></a>8.Handler处理器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">为什么要学习handler？</span><br><span class="line">	urllib.request.urlopen(url)</span><br><span class="line">		不能定制请求头</span><br><span class="line">	urllib.request.Request(url,headers,data)</span><br><span class="line">		可以定制请求头</span><br><span class="line"></span><br><span class="line">    Handler</span><br><span class="line">    	定制更高级的请求头（随着业务逻辑的复杂 请求对象的定制已经满足不了我们的需求（动态cookie和代理不能使用请求对象的定制）</span><br></pre></td></tr></table></figure>



<ul>
<li>案例：handler的基本使用</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.48&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># handler   build_opener    open</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#（1）获取handler对象</span></span><br><span class="line">handler = urllib.request.HTTPHandler()</span><br><span class="line"><span class="comment">#（2）获取opener对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"><span class="comment">#（3）调用open方法</span></span><br><span class="line">response = opener.<span class="built_in">open</span>(request)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>



<h2 id="9-代理服务器"><a href="#9-代理服务器" class="headerlink" title="9.代理服务器"></a>9.代理服务器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>代理的常用功能?</span><br><span class="line">    <span class="number">1.</span>突破自身IP访问限制，访问国外站点。</span><br><span class="line">    <span class="number">2.</span>访问一些单位或团体内部资源</span><br><span class="line">        扩展：某大学FTP(前提是该代理地址在该资源的允许访问范围之内)，使用教育网内地址段免费代理服务器，就可以用于对教育网开放的各类FTP下载上传，以及各类资料查询共享等服务。</span><br><span class="line">    <span class="number">3.</span>提高访问速度</span><br><span class="line">    	扩展：通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同的信息时， 则直接由缓冲区中取出信息，传给用户，以提高访问速度。</span><br><span class="line">	<span class="number">4.</span>隐藏真实IP</span><br><span class="line">		扩展：上网者也可以通过这种方法隐藏自己的IP，免受攻击。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span>代码配置代理</span><br><span class="line">    创建Reuqest对象</span><br><span class="line">    创建ProxyHandler对象</span><br><span class="line">    用handler对象创建opener对象</span><br><span class="line">    使用opener.<span class="built_in">open</span>函数发送请求</span><br></pre></td></tr></table></figure>



<ul>
<li>案例1：快代理</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.ip138.com/&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.48&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># response = urllib.request.urlopen(request)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># https://www.kuaidaili.com/free/ 快代理的网址</span></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">&#x27;http:&#x27;</span>:<span class="string">&#x27;175.167.21.80:23443&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">handler = urllib.request.ProxyHandler()</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(request)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;daili.html&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(content)</span><br></pre></td></tr></table></figure>



<ul>
<li>案例2：代理池</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">proxies_pool = [</span><br><span class="line">    &#123;<span class="string">&#x27;http:&#x27;</span>:<span class="string">&#x27;175.167.21.80:23443&#x27;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;http:&#x27;</span>:<span class="string">&#x27;175.167.21.80:23443&#x27;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;http:&#x27;</span>:<span class="string">&#x27;175.167.21.80:23443&#x27;</span>&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">proxies = random.choice(proxies_pool)</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.ip138.com/&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.48&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line">handler = urllib.request.ProxyHandler()</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(request)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;daili.html&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(content)</span><br></pre></td></tr></table></figure>





<h1 id="3-解析"><a href="#3-解析" class="headerlink" title="3.解析"></a>3.解析</h1><h2 id="1-xpath"><a href="#1-xpath" class="headerlink" title="1.xpath"></a>1.xpath</h2><ul>
<li>安装xpath</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">xpath使用：</span><br><span class="line">    注意：提前安装xpath插件</span><br><span class="line">    （<span class="number">1</span>）ctrl + shift + x 启动扩展</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span>安装lxml库</span><br><span class="line">	pip install lxml -i https://pypi.douban.com/simple</span><br><span class="line"><span class="number">2.</span>导入lxml.etree</span><br><span class="line">	<span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="number">3.</span>etree.parse() 解析本地文件</span><br><span class="line">    html_tree = etree.parse(<span class="string">&#x27;XX.html&#x27;</span>)</span><br><span class="line"><span class="number">4.</span>etree.HTML() 服务器响应文件</span><br><span class="line">    html_tree = etree.HTML(response.read().decode(<span class="string">&#x27;utf‐8&#x27;</span>)</span><br><span class="line"><span class="number">4.</span>语法的格式：</span><br><span class="line">	html_tree.xpath(xpath路径)</span><br></pre></td></tr></table></figure>



<ul>
<li>xpath的基本语法</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>路径查询</span><br><span class="line">    //：查找所有子孙节点，不考虑层级关系</span><br><span class="line">    / ：找直接子节点</span><br><span class="line"><span class="number">2.</span>谓词查询</span><br><span class="line">    //div[@<span class="built_in">id</span>]</span><br><span class="line">    //div[@<span class="built_in">id</span>=<span class="string">&quot;maincontent&quot;</span>]</span><br><span class="line"><span class="number">3.</span>属性查询</span><br><span class="line">    //@<span class="keyword">class</span></span><br><span class="line"><span class="number">4.</span>模糊查询</span><br><span class="line">    //div[contains(@<span class="built_in">id</span>, <span class="string">&quot;he&quot;</span>)]</span><br><span class="line">    //div[starts‐<span class="keyword">with</span>(@<span class="built_in">id</span>, <span class="string">&quot;he&quot;</span>)]</span><br><span class="line"><span class="number">5.</span>内容查询</span><br><span class="line">    //div/h1/text()</span><br><span class="line"><span class="number">6.</span>逻辑运算</span><br><span class="line">    //div[@<span class="built_in">id</span>=<span class="string">&quot;head&quot;</span> <span class="keyword">and</span> @<span class="keyword">class</span>=<span class="string">&quot;s_down&quot;</span>]</span><br><span class="line">    //title | //price</span><br></pre></td></tr></table></figure>



<ul>
<li><p>例子演示：</p>
<ol>
<li><p>数据的准备</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">id</span>=<span class="string">&quot;l1&quot;</span> <span class="attr">class</span>=<span class="string">&quot;c1&quot;</span>&gt;</span>北京<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">id</span>=<span class="string">&quot;l2&quot;</span>&gt;</span>上海<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">id</span>=<span class="string">&quot;c3&quot;</span>&gt;</span>深圳<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">id</span>=<span class="string">&quot;c4&quot;</span>&gt;</span>武汉<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span>&gt;</span>大连<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span>&gt;</span>锦州<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span>&gt;</span>沈阳<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>演示的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">xpath解析</span></span><br><span class="line"><span class="string">    (1)本地文件</span></span><br><span class="line"><span class="string">        html_tree = etree.parse(&#x27;XX.html&#x27;)</span></span><br><span class="line"><span class="string">    (2)服务器相应的数据</span></span><br><span class="line"><span class="string">        html_tree = etree.HTML(response.read().decode(&#x27;utf‐8&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># xpath解析本地文件</span></span><br><span class="line">tree = etree.parse(<span class="string">&#x27;1.解析_xpath的基本使用.html&#x27;</span>)</span><br><span class="line"><span class="comment"># tree.xpath(&#x27;xpath的路径&#x27;)</span></span><br><span class="line"><span class="comment"># （1）查找ul下的li</span></span><br><span class="line">li_list = tree.xpath(<span class="string">&#x27;//body/ul/li&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(li_list))</span><br><span class="line"><span class="comment"># （2）谓词查询 需求：查找所有id属性的li标签</span></span><br><span class="line">li_list = tree.xpath(<span class="string">&#x27;//ul/li[@id]&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(li_list))</span><br><span class="line"><span class="comment"># （3）查找id为l1的li标签</span></span><br><span class="line">li_list = tree.xpath(<span class="string">&#x27;//ul/li[@id=&quot;l1&quot;]/text()&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(li_list)</span><br><span class="line"><span class="comment"># （4）查找到id为l1的li标签的class的属性值</span></span><br><span class="line">li = tree.xpath(<span class="string">&#x27;//ul/li[@id=&quot;l1&quot;]/@class&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(li,<span class="built_in">len</span>(li))</span><br><span class="line"><span class="comment"># （5）查询id中包含l的li标签</span></span><br><span class="line">li = tree.xpath(<span class="string">&#x27;//ul/li[contains(@id, &quot;l&quot;)]/text()&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(li,<span class="built_in">len</span>(li))</span><br><span class="line"><span class="comment"># （6）查询id的值以l开头的li标签</span></span><br><span class="line">li = tree.xpath(<span class="string">&#x27;//ul/li[starts-with(@id,&quot;c&quot;)]/text()&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(li,<span class="built_in">len</span>(li))</span><br><span class="line"><span class="comment"># （7）逻辑运算 查询id为l1和class为c1的</span></span><br><span class="line">li = tree.xpath(<span class="string">&#x27;//ul/li[@id=&quot;l1&quot; and @class=&quot;c1&quot;]/text()&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(li,<span class="built_in">len</span>(li))</span><br><span class="line"></span><br><span class="line">li = tree.xpath(<span class="string">&#x27;//ul/li[@id=&quot;l1&quot;]/text() | //ul/li[@id=&quot;l2&quot;]/text()&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(li,<span class="built_in">len</span>(li))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>案例1：获取百度网站的:”百度一下”</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">需求：</span></span><br><span class="line"><span class="string">获取百度一下这4个字</span></span><br><span class="line"><span class="string">1.获取网页的源码</span></span><br><span class="line"><span class="string">2.解析</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.48&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析网页的源码 获取我们想要的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析服务器响应的文件</span></span><br><span class="line">tree = etree.HTML(content)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取想要的数据，xpath返回的数据为列表数据类型</span></span><br><span class="line">result = tree.xpath(<span class="string">&#x27;//input[@id=&quot;su&quot;]/@value&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>案例：站长素材</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">需求：https://sc.chinaz.com/下载前5页的地址</span></span><br><span class="line"><span class="string">第1页：https://sc.chinaz.com/tupian/qinglvtupian.html</span></span><br><span class="line"><span class="string">第2页：https://sc.chinaz.com/tupian/qinglvtupian_2.html</span></span><br><span class="line"><span class="string">第3页：https://sc.chinaz.com/tupian/qinglvtupian_3.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">（1）请求对象的定制</span></span><br><span class="line"><span class="string">（2）获取网页的源码</span></span><br><span class="line"><span class="string">（3）下载</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_request</span>(<span class="params">page</span>):</span><br><span class="line">    <span class="keyword">if</span> page == <span class="number">1</span>:</span><br><span class="line">        url = <span class="string">&#x27;https://sc.chinaz.com/tupian/qinglvtupian.html&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        url = <span class="string">&#x27;https://sc.chinaz.com/tupian/qinglvtupian_&#x27;</span> + <span class="built_in">str</span>(page) +<span class="string">&#x27;.html&#x27;</span></span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.48&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">request</span>):</span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_load</span>(<span class="params">content</span>):</span><br><span class="line">    <span class="comment"># 下载图片  urllib.request.urlretrieve(&#x27;图片的地址&#x27;,&#x27;图片的名字&#x27;)</span></span><br><span class="line">    tree = etree.HTML(content)</span><br><span class="line"></span><br><span class="line">    name_list = tree.xpath(<span class="string">&#x27;//div//img/@alt&#x27;</span>)</span><br><span class="line">    <span class="comment"># 一般涉及到图片的时候都会进行懒加载（可能路径会发生变化）</span></span><br><span class="line">    Img_list = tree.xpath(<span class="string">&#x27;//div[@class=&quot;container&quot;]//img/@data-original&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(name_list)):</span><br><span class="line">        url = <span class="string">&#x27;https:&#x27;</span> + Img_list[i]</span><br><span class="line">        name = <span class="string">&#x27;./123/&#x27;</span> + name_list[i] + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">        urllib.request.urlretrieve(url,name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;请输入起始的页码：&quot;</span>))</span><br><span class="line">    end = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;请输入结束的页码：&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start,end+<span class="number">1</span>):</span><br><span class="line">        request = create_request(page)</span><br><span class="line">        content = get_content(request)</span><br><span class="line">        down_load(content)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;第%d页下载完成&quot;</span>%(page))</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="2-JsonPath"><a href="#2-JsonPath" class="headerlink" title="2.JsonPath"></a>2.JsonPath</h2><blockquote>
<ul>
<li>只能识别本地文件</li>
</ul>
</blockquote>
<ol>
<li><p>安装与使用</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/luxideyao/article/details/77802389">教程链接</a></p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip安装：</span><br><span class="line">	pip install jsonpath</span><br><span class="line">jsonpath的使用：</span><br><span class="line">    obj = json.load(open(&#x27;json文件路径&#x27;, &#x27;r&#x27;, encoding=&#x27;utf‐8&#x27;))</span><br><span class="line">    ret = jsonpath.jsonpath(obj, &#x27;jsonpath语法&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/silent-wuhen/Blog_picture01/H04_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%8C%96%E6%8E%98/4.python/5.python%E7%88%AC%E8%99%AB/001.png?raw=true" alt="001.png"></p>
</li>
<li><p>案例（淘票票）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://dianying.taobao.com/cityAction.json?activityId&amp;_ksTS=1629789477003_137&amp;jsoncallback=jsonp138&amp;action=cityAction&amp;n_s=new&amp;event_submit_doGetAllRegion=true&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="comment"># &#x27;:authority&#x27;: &#x27;dianying.taobao.com&#x27;,</span></span><br><span class="line">    <span class="comment"># &#x27;:method&#x27;: &#x27;GET&#x27;,</span></span><br><span class="line">    <span class="comment"># &#x27;:path&#x27;: &#x27;/cityAction.json?activityId&amp;_ksTS=1629789477003_137&amp;jsoncallback=jsonp138&amp;action=cityAction&amp;n_s=new&amp;event_submit_doGetAllRegion=true&#x27;,</span></span><br><span class="line">    <span class="comment"># &#x27;:scheme&#x27;: &#x27;https&#x27;,</span></span><br><span class="line">    <span class="string">&#x27;accept&#x27;</span>: <span class="string">&#x27;text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01&#x27;</span>,</span><br><span class="line">    <span class="comment"># &#x27;accept-encoding&#x27;: &#x27;gzip, deflate, br&#x27;,</span></span><br><span class="line">    <span class="string">&#x27;accept-language&#x27;</span>: <span class="string">&#x27;zh-CN,zh;q=0.9&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;cookie&#x27;</span>: <span class="string">&#x27;cna=UkO6F8VULRwCAXTqq7dbS5A8; miid=949542021157939863; sgcookie=E100F01JK9XMmyoZRigjfmZKExNdRHQqPf4v9NIWIC1nnpnxyNgROLshAf0gz7lGnkKvwCnu1umyfirMSAWtubqc4g%3D%3D; tracknick=action_li; _cc_=UIHiLt3xSw%3D%3D; enc=dA18hg7jG1xapfVGPHoQCAkPQ4as1%2FEUqsG4M6AcAjHFFUM54HWpBv4AAm0MbQgqO%2BiZ5qkUeLIxljrHkOW%2BtQ%3D%3D; hng=CN%7Czh-CN%7CCNY%7C156; thw=cn; _m_h5_tk=3ca69de1b9ad7dce614840fcd015dcdb_1629776735568; _m_h5_tk_enc=ab56df54999d1d2cac2f82753ae29f82; t=874e6ce33295bf6b95cfcfaff0af0db6; xlly_s=1; cookie2=13acd8f4dafac4f7bd2177d6710d60fe; v=0; _tb_token_=e65ebbe536158; tfstk=cGhRB7mNpnxkDmUx7YpDAMNM2gTGZbWLxUZN9U4ulewe025didli6j5AFPI8MEC..; l=eBrgmF1cOsMXqSxaBO5aFurza77tzIRb8sPzaNbMiInca6OdtFt_rNCK2Ns9SdtjgtfFBetPVKlOcRCEF3apbgiMW_N-1NKDSxJ6-; isg=BBoas2yXLzHdGp3pCh7XVmpja8A8S54lyLj1RySTHq14l7vRDNufNAjpZ2MLRxa9&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;referer&#x27;</span>: <span class="string">&#x27;https://dianying.taobao.com/&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua&#x27;</span>: <span class="string">&#x27;&quot;Chromium&quot;;v=&quot;92&quot;, &quot; Not A;Brand&quot;;v=&quot;99&quot;, &quot;Google Chrome&quot;;v=&quot;92&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-mobile&#x27;</span>: <span class="string">&#x27;?0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-dest&#x27;</span>: <span class="string">&#x27;empty&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-mode&#x27;</span>: <span class="string">&#x27;cors&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-site&#x27;</span>: <span class="string">&#x27;same-origin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;x-requested-with&#x27;</span>: <span class="string">&#x27;XMLHttpRequest&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># print(content)</span></span><br><span class="line"></span><br><span class="line">content = content.split(<span class="string">&#x27;(&#x27;</span>)[<span class="number">1</span>].split(<span class="string">&#x27;)&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;5._解析_jsonpath解析淘票票.json&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> jsonpath</span><br><span class="line">obj = json.load(<span class="built_in">open</span>(<span class="string">&#x27;5._解析_jsonpath解析淘票票.json&#x27;</span>,<span class="string">&#x27;r&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line"></span><br><span class="line">city_list = jsonpath.jsonpath(obj,<span class="string">&#x27;$..regionName&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(city_list)</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="3-BeautifulSoup"><a href="#3-BeautifulSoup" class="headerlink" title="3.BeautifulSoup"></a>3.BeautifulSoup</h2><ol>
<li><p>基本简介</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.BeautifulSoup简称：</span><br><span class="line">	bs4</span><br><span class="line">2.什么是BeatifulSoup？</span><br><span class="line">    BeautifulSoup，和lxml一样，是一个html的解析器，主要功能也是解析和提取数据</span><br><span class="line">3.优缺点？</span><br><span class="line">    缺点：效率没有lxml的效率高</span><br><span class="line">    优点：接口设计人性化，使用方便</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装以及创建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>安装</span><br><span class="line">    pip install bs4</span><br><span class="line"><span class="number">2.</span>导入</span><br><span class="line">    <span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="number">3.</span>创建对象</span><br><span class="line">    服务器响应的文件生成对象</span><br><span class="line">        soup = BeautifulSoup(response.read().decode(), <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    本地文件生成对象</span><br><span class="line">        soup = BeautifulSoup(<span class="built_in">open</span>(<span class="string">&#x27;6.解析_bs4的基本使用.html&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>),<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">        注意：默认打开文件的编码格式gbk所以需要指定打开编码格式</span><br></pre></td></tr></table></figure>
</li>
<li><p>节点定位</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">1.根据标签名查找节点</span><br><span class="line">    soup.a 【注】只能找到第一个a</span><br><span class="line">        soup.a.name</span><br><span class="line">        soup.a.attrs # 获取标签的属性和属性值</span><br><span class="line">2.函数</span><br><span class="line">    (1).find(返回一个对象)</span><br><span class="line">        find(&#x27;a&#x27;)：只找到第一个a标签</span><br><span class="line">    (2).find_all(返回一个列表)</span><br><span class="line">        find_all(&#x27;a&#x27;) 查找到所有的a</span><br><span class="line">        find_all([&#x27;a&#x27;, &#x27;span&#x27;]) 返回所有的a和span</span><br><span class="line">        find_all(&#x27;a&#x27;, limit=2) 只找前两个a</span><br><span class="line">    (3).select(根据选择器得到节点对象)【推荐】</span><br><span class="line">        1.element</span><br><span class="line">            eg:p</span><br><span class="line">        2..class</span><br><span class="line">            eg:.firstname</span><br><span class="line">        3.#id</span><br><span class="line">            eg:#firstname</span><br><span class="line">        4.属性选择器</span><br><span class="line">            [attribute]</span><br><span class="line">                eg:li = soup.select(&#x27;li[class]&#x27;)</span><br><span class="line">            [attribute=value]</span><br><span class="line">                eg:li = soup.select(&#x27;li[class=&quot;hengheng1&quot;]&#x27;)</span><br><span class="line">        5.层级选择器</span><br><span class="line">            element element</span><br><span class="line">                div p</span><br><span class="line">            element&gt;element</span><br><span class="line">                div&gt;p</span><br><span class="line">            element,element</span><br><span class="line">                div,p</span><br><span class="line">                    eg:soup = soup.select(&#x27;a,span&#x27;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>节点信息</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(1).获取节点内容：适用于标签中嵌套标签的结构</span><br><span class="line">    obj.string</span><br><span class="line">    obj.get_text()【推荐】</span><br><span class="line">(2).节点的属性</span><br><span class="line">    tag.name 获取标签名</span><br><span class="line">        eg:tag = find(&#x27;li)</span><br><span class="line">            print(tag.name)</span><br><span class="line">    tag.attrs将属性值作为一个字典返回</span><br><span class="line">(3).获取节点属性</span><br><span class="line">    obj.attrs.get(&#x27;title&#x27;)【常用】</span><br><span class="line">    obj.get(&#x27;title&#x27;)</span><br><span class="line">    obj[&#x27;title&#x27;]</span><br></pre></td></tr></table></figure>
</li>
<li><p>案例(星巴克数据)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 时间问题,代码已经失效 </span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.starbucks.com.cn/menu/&#x27;</span></span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(url)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(content,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># //ul[@class=&quot;grid padded-3 product&quot;]//strong/text()</span></span><br><span class="line">name_list = soup.select(<span class="string">&#x27;ul[class=&quot;grid padded-3 product&quot;] strong&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> name_list:</span><br><span class="line">    <span class="built_in">print</span>(name.get_text())</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="4-selenium"><a href="#4-selenium" class="headerlink" title="4.selenium"></a>4.selenium</h1><h2 id="1-selenium概述"><a href="#1-selenium概述" class="headerlink" title="1.selenium概述"></a>1.selenium概述</h2><ol>
<li><p>selenium概念</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（1）Selenium是一个用于Web应用程序测试的工具。</span><br><span class="line">（2）Selenium 测试直接运行在浏览器中，就像真正的用户在操作一样。</span><br><span class="line">（3）支持通过各种driver（FirfoxDriver，IternetExplorerDriver，OperaDriver，ChromeDriver）驱动真实浏览器完成测试。</span><br><span class="line">（4）selenium也是支持无界面浏览器操作的。</span><br></pre></td></tr></table></figure>
</li>
<li><p>为什么使用selenium</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">模拟浏览器功能，自动执行网页中的js代码，实现动态加载</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装selenium</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">（1）操作谷歌浏览器驱动下载地址</span><br><span class="line">    http://chromedriver.storage.googleapis.com/index.html</span><br><span class="line">（2）谷歌驱动和谷歌浏览器版本之间的映射表</span><br><span class="line">    http://blog.csdn.net/huilan_same/article/details/51896672</span><br><span class="line">（3）查看谷歌浏览器版本</span><br><span class="line">    谷歌浏览器右上角‐‐&gt;帮助‐‐&gt;关于</span><br><span class="line">（4）pip install selenium</span><br></pre></td></tr></table></figure>
</li>
<li><p>selenium的使用步骤</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">（1）导入：from selenium import webdriver</span><br><span class="line">（2）创建谷歌浏览器操作对象：</span><br><span class="line">    path = 谷歌浏览器驱动文件路径</span><br><span class="line">    browser = webdriver.Chrome(path)</span><br><span class="line">（3）访问网址</span><br><span class="line">    url = 要访问的网址</span><br><span class="line">    browser.get(url)</span><br></pre></td></tr></table></figure>
</li>
<li><p>selenium的元素定位</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">元素定位：自动化要做的就是模拟鼠标和键盘来操作来操作这些元素，点击、输入等等。操作这些元素前首先要找到它们，WebDriver提供很多定位元素的方法</span><br><span class="line"></span><br><span class="line">方法：</span><br><span class="line">1.find_element_by_id</span><br><span class="line">    eg:button = browser.find_element_by_id(&#x27;su&#x27;)</span><br><span class="line">2.find_elements_by_name</span><br><span class="line">    eg:name = browser.find_element_by_name(&#x27;wd&#x27;)</span><br><span class="line">3.find_elements_by_xpath</span><br><span class="line">    eg:xpath1 = browser.find_elements_by_xpath(&#x27;//input[@id=&quot;su&quot;]&#x27;)</span><br><span class="line">4.find_elements_by_tag_name</span><br><span class="line">    eg:names = browser.find_elements_by_tag_name(&#x27;input&#x27;)</span><br><span class="line">5.find_elements_by_css_selector</span><br><span class="line">    eg:my_input = browser.find_elements_by_css_selector(&#x27;#kw&#x27;)[0]</span><br><span class="line">6.find_elements_by_link_text</span><br><span class="line">    eg:browser.find_element_by_link_text(&quot;新闻&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>访问元素信息</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.获取元素属性</span><br><span class="line">    .get_attribute(&#x27;class&#x27;)</span><br><span class="line">2.获取元素文本</span><br><span class="line">    .text</span><br><span class="line">3.获取标签名</span><br><span class="line">    .tag_name</span><br></pre></td></tr></table></figure>
</li>
<li><p>交互</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">点击:click()</span><br><span class="line">输入:send_keys()</span><br><span class="line">后退操作:browser.back()</span><br><span class="line">前进操作:browser.forword()</span><br><span class="line">模拟JS滚动:</span><br><span class="line">    js=&#x27;document.documentElement.scrollTop=100000&#x27;</span><br><span class="line">    browser.execute_script(js) 执行js代码</span><br><span class="line">获取网页代码：page_source</span><br><span class="line">退出：browser.quit()</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-Chrome-handless"><a href="#2-Chrome-handless" class="headerlink" title="2.Chrome handless"></a>2.Chrome handless</h2><p>Chrome-headless 模式， Google 针对 Chrome 浏览器 59版 新增加的一种模式，可以不打开UI界面的情况下使用 Chrome 浏览器，所以运行效果与 Chrome 保持完美一致</p>
<ol>
<li><p>系统要求：</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Chrome</span><br><span class="line">    Unix\Linux 系统需要 chrome &gt;= 59</span><br><span class="line">    Windows 系统需要 chrome &gt;= 60</span><br><span class="line">Python3.6</span><br><span class="line">Selenium==3.4.*</span><br><span class="line">ChromeDriver==2.31</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line"></span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_argument(<span class="string">&#x27;‐‐headless&#x27;</span>)</span><br><span class="line">chrome_options.add_argument(<span class="string">&#x27;‐‐disable‐gpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">path = <span class="string">r&#x27;C:\Program Files (x86)\Google\Chrome\Application\chrome.exe&#x27;</span></span><br><span class="line">chrome_options.binary_location = path</span><br><span class="line">browser = webdriver.Chrome(chrome_options=chrome_options)</span><br><span class="line"></span><br><span class="line">browser.get(<span class="string">&#x27;http://www.baidu.com/&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置封装：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="comment">#这个是浏览器自带的 不需要再做额外的操作</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">share_browser</span>():</span><br><span class="line">    <span class="comment">#初始化</span></span><br><span class="line">    chrome_options = Options()</span><br><span class="line">    chrome_options.add_argument(<span class="string">&#x27;‐‐headless&#x27;</span>)</span><br><span class="line">    chrome_options.add_argument(<span class="string">&#x27;‐‐disable‐gpu&#x27;</span>)</span><br><span class="line">    <span class="comment">#浏览器的安装路径 打开文件位置</span></span><br><span class="line">    <span class="comment">#这个路径是你谷歌浏览器的路径</span></span><br><span class="line">    path = <span class="string">r&#x27;C:\Program Files (x86)\Google\Chrome\Application\chrome.exe&#x27;</span></span><br><span class="line">    chrome_options.binary_location = path</span><br><span class="line">    browser = webdriver.Chrome(chrome_options=chrome_options)</span><br><span class="line">    <span class="keyword">return</span> browser</span><br></pre></td></tr></table></figure>
</li>
<li><p>封装调用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> handless <span class="keyword">import</span> share_browser</span><br><span class="line"></span><br><span class="line">browser = share_browser()</span><br><span class="line"></span><br><span class="line">browser.get(<span class="string">&#x27;http://www.baidu.com/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">browser.save_screenshot(<span class="string">&#x27;handless1.png&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="5-requests"><a href="#5-requests" class="headerlink" title="5.requests"></a>5.requests</h1><h2 id="1-基本使用"><a href="#1-基本使用" class="headerlink" title="1.基本使用"></a>1.基本使用</h2><ol>
<li><p>文档</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">官方文档</span><br><span class="line">    http://cn.python‐requests.org/zh_CN/latest/</span><br><span class="line">快速上手</span><br><span class="line">    http://cn.python‐requests.org/zh_CN/latest/user/quickstart.html</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br></pre></td></tr></table></figure>
</li>
<li><p>response的属性以及类型</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">类型			   :models.Response</span><br><span class="line">r.text			:获取网站源码</span><br><span class="line">r.encoding 		：访问或定制编码方式</span><br><span class="line">r.url 			：获取请求的url</span><br><span class="line">r.content 		：响应的字节类型</span><br><span class="line">r.status_code 	：响应的状态码</span><br><span class="line">r.headers 		：响应的头信息</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-get请求"><a href="#2-get请求" class="headerlink" title="2.get请求"></a>2.get请求</h2><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">requests.get()</span><br><span class="line"></span><br><span class="line">eg:</span><br><span class="line">import requests</span><br><span class="line">url = &#x27;http://www.baidu.com/s?&#x27;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&#x27;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    &#x27;wd&#x27;:&#x27;北京&#x27;</span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url,params=data,headers=headers)</span><br><span class="line">    </span><br><span class="line">定制参数</span><br><span class="line">    参数使用params传递</span><br><span class="line">    参数无需urlencode编码</span><br><span class="line">    不需要请求对象的定制</span><br><span class="line">    请求资源路径中？可加可不加</span><br></pre></td></tr></table></figure>



<h2 id="3-post请求"><a href="#3-post请求" class="headerlink" title="3.post请求"></a>3.post请求</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">requests.post()</span><br><span class="line"></span><br><span class="line">百度翻译:</span><br><span class="line">eg:</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">post_url = <span class="string">&#x27;http://fanyi.baidu.com/sug&#x27;</span></span><br><span class="line">headers=&#123;</span><br><span class="line">    <span class="string">&#x27;User‐Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;kw&#x27;</span>: <span class="string">&#x27;eye&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">r = requests.post(url = post_url,headers=headers,data=data)</span><br></pre></td></tr></table></figure>

<ul>
<li>get和post区别？<ol>
<li>get请求的参数名字是params post请求的参数的名字是data</li>
<li>请求资源路径后面可以不加?</li>
<li>不需要手动编解码</li>
<li>不需要做请求对象的定制</li>
</ol>
</li>
</ul>
<h2 id="4-代理"><a href="#4-代理" class="headerlink" title="4.代理"></a>4.代理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">proxy定制</span></span><br><span class="line"><span class="string">    在请求中设置proxies参数</span></span><br><span class="line"><span class="string">    参数类型是一个字典类型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">eg:</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;user‐agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML,like Gecko) Chrome/65.0.3325.181 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;wd&#x27;</span>:<span class="string">&#x27;ip&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">proxy = &#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>:<span class="string">&#x27;219.149.59.250:9797&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(url=url,params=data,headers=headers,proxies=proxy)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;proxy.html&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf‐8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(r.text)</span><br></pre></td></tr></table></figure>



<h2 id="5-cookie定制"><a href="#5-cookie定制" class="headerlink" title="5.cookie定制"></a>5.cookie定制</h2><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">应用案例：</span><br><span class="line">（1）古诗文网（需要验证）</span><br><span class="line">（2）云打码平台</span><br><span class="line">    用户登陆 actionuser action</span><br><span class="line">    开发者登陆 actioncode action</span><br></pre></td></tr></table></figure>



<h1 id="6-scrapy"><a href="#6-scrapy" class="headerlink" title="6.scrapy"></a>6.scrapy</h1><h2 id="1-scrapy概述"><a href="#1-scrapy概述" class="headerlink" title="1.scrapy概述"></a>1.scrapy概述</h2><h3 id="1-认识scrapy"><a href="#1-认识scrapy" class="headerlink" title="1.认识scrapy"></a>1.认识scrapy</h3><ol>
<li><p>scrapy是什么？</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装scrapy：</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br><span class="line"></span><br><span class="line">(1)安装出错,且报错信息：</span><br><span class="line">    pip install Scrapy</span><br><span class="line">    building &#x27;twisted.test.raiser&#x27; extension</span><br><span class="line">    error: Microsoft Visual C++ 14.0 is required. Get it with &quot;Microsoft Visual C++ Build Tools&quot;: http://landinghub.visualstudio.com/visual‐cpp‐build‐tools</span><br><span class="line">    </span><br><span class="line">解决方案：</span><br><span class="line">    1.进入：http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</span><br><span class="line">    2.下载twisted对应版本的whl文件（如Twisted‐17.5.0‐cp36‐cp36m‐win_amd64.whl），cp后面是    python版本，amd64代表64位</span><br><span class="line">    3.运行命令：</span><br><span class="line">    pip install C:\Users\...\Twisted‐17.5.0‐cp36‐cp36m‐win_amd64.whl # 本地文件</span><br><span class="line">    pip install Scrapy</span><br><span class="line">    </span><br><span class="line">(2)若再报错</span><br><span class="line">python ‐m pip install ‐‐upgrade pip</span><br><span class="line"></span><br><span class="line">(3)若再报错：win32</span><br><span class="line">解决方法：</span><br><span class="line">pip install pypiwin32</span><br><span class="line"></span><br><span class="line">(4)若再报错：使用anaconda</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-scrapy项目的创建以及运行"><a href="#2-scrapy项目的创建以及运行" class="headerlink" title="2.scrapy项目的创建以及运行"></a>2.scrapy项目的创建以及运行</h3><ol>
<li><p>创建scrapy项目，在终端输入：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject 项目名称</span><br></pre></td></tr></table></figure>
</li>
<li><p>项目组成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spiders <span class="comment"># 存储爬虫文件</span></span><br><span class="line">    __init__.py</span><br><span class="line">    自定义的爬虫文件.py <span class="comment"># 由我们自己创建，是实现爬虫核心功能的文件</span></span><br><span class="line">__init__.py</span><br><span class="line">items.py <span class="comment"># 定义数据结构的地方，是一个继承自scrapy.Item的类</span></span><br><span class="line">middlewares.py <span class="comment"># 中间件,代理</span></span><br><span class="line">pipelines.py <span class="comment"># 管道文件，里面只有一个类，用于处理下载数据的后续处理[默认是300优先级，值越小优先级越高（1‐1000）]</span></span><br><span class="line">settings.py <span class="comment"># 配置文件 比如：是否遵守robots协议，User‐Agent定义等</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>创建爬虫文件</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">创建爬虫文件：</span><br><span class="line">    （1）跳转到spiders文件夹：cd 目录名字/目录名字/spiders</span><br><span class="line">    （2）scrapy genspider 爬虫名字 网页的域名（一般不写http://）</span><br><span class="line">    	eg:scrapy genspider baidu www.baidu.com</span><br><span class="line"></span><br><span class="line">爬虫文件的基本组成(继承scrapy.Spider类)：</span><br><span class="line">    name = &#x27;baidu&#x27; # 运行爬虫文件时使用的名字</span><br><span class="line">    allowed_domains # 爬虫允许的域名，在爬取的时候，如果不是此域名之下的url，会被过滤掉</span><br><span class="line">    start_urls # 声明了爬虫的起始地址，可以写多个url，一般只有一个。注：html的url最后不能有/</span><br><span class="line">    parse(self, response) # 解析数据的回调函数</span><br><span class="line">        response.text # 响应的是字符串</span><br><span class="line">        response.body # 响应的是二进制文件</span><br><span class="line">        response.xpath() # xpath方法的返回值类型是selector列表</span><br><span class="line">        extract() # 提取的是selector对象的是data</span><br><span class="line">        extract_first() # 提取的是selector列表中的第一个数据</span><br></pre></td></tr></table></figure>
</li>
<li><p>运行爬虫文件，cmd输入：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl 爬虫名称</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">注意：应在spiders文件夹内执行</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="3-scrapy架构组成"><a href="#3-scrapy架构组成" class="headerlink" title="3.scrapy架构组成"></a>3.scrapy架构组成</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">（1）引擎：自动运行，无需关注，会自动组织所有的请求对象，分发给下载器</span><br><span class="line">（2）下载器：从引擎处获取到请求对象后，请求数据</span><br><span class="line">（3）spiders：Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。</span><br><span class="line">（4）调度器：有自己的调度规则，无需关注</span><br><span class="line">（5）管道（Item pipeline）：最终处理数据的管道，会预留接口供处理数据</span><br><span class="line">  当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。每个item pipeline组件(有时称之为“Item Pipeline”)是实现了简单方法的Python类。他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理。</span><br><span class="line">以下是item pipeline的一些典型应用：</span><br><span class="line">    1. 清理HTML数据</span><br><span class="line">    2. 验证爬取的数据(检查item包含某些字段)</span><br><span class="line">    3. 查重(并丢弃)</span><br><span class="line">    4. 将爬取结果保存到数据库中</span><br></pre></td></tr></table></figure>



<h3 id="4-scrapy工作原理"><a href="#4-scrapy工作原理" class="headerlink" title="4.scrapy工作原理"></a>4.scrapy工作原理</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/silent-wuhen/Blog_picture01/H04_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%8C%96%E6%8E%98/4.python/5.python%E7%88%AC%E8%99%AB/002.png?raw=true" alt="001.png"></p>
<h2 id="2-scrapy-shell"><a href="#2-scrapy-shell" class="headerlink" title="2.scrapy shell"></a>2.scrapy shell</h2><ol>
<li><p>scrapy shell介绍</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Scrapy终端，是一个交互终端，在未启动spider的情况下尝试及调试爬取代码。 其本意是用来测试提取数据的代码，可将其作为正常的Python终端，在上面测试任何的Python代码。</span><br><span class="line">该终端是用来测试XPath或CSS表达式，查看他们的工作方式及从爬取的网页中提取的数据。在编写spider时，该终端提供了交互性测试表达式代码的功能，免去了每次修改后运行spider的麻烦。一旦熟悉了Scrapy终端后，其在开发和调试spider时发挥的巨大作用。</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装ipython</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install ipython</span><br></pre></td></tr></table></figure>
</li>
<li><p>应用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(1)scrapy shell www.baidu.com</span><br><span class="line">(2)scrapy shell http://www.baidu.com</span><br><span class="line">(3) scrapy shell &quot;http://www.baidu.com&quot;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="3-yield"><a href="#3-yield" class="headerlink" title="3.yield"></a>3.yield</h2><h3 id="1-概念-2"><a href="#1-概念-2" class="headerlink" title="1.概念"></a>1.概念</h3><ol>
<li>带有 yield 的函数不再是一个普通函数，而是一个生成器generator，可用于迭代</li>
<li>yield 是一个类似 return 的关键字，迭代一次遇到yield时就返回yield后面(右边)的值。重点是：下一次迭代时，从上一次迭代遇到的yield后面的代码(下一行)开始执行</li>
<li>简要理解：yield就是 return 返回一个值，并且记住这个返回的位置，下次迭代就从这个位置后(下一行)开始</li>
</ol>
<h3 id="2-案例1（按页下载）：爬取汇书网"><a href="#2-案例1（按页下载）：爬取汇书网" class="headerlink" title="2.案例1（按页下载）：爬取汇书网"></a>2.案例1（按页下载）：爬取<a target="_blank" rel="noopener" href="https://www.huibooks.com/wx">汇书网</a></h3><ol>
<li><p>创建项目：切换到项目存放目录，创建项目。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd C:\Users\wuhen\Desktop</span><br><span class="line"></span><br><span class="line">scrapy startproject hsw</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建爬虫文件：进入爬虫存储文件，创建爬虫。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd hsw/hsw/spiders</span><br><span class="line"></span><br><span class="line">scrapy genspider hsw_spider https://www.huibooks.com/wx</span><br></pre></td></tr></table></figure>
</li>
<li><p>爬虫运行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">运行代码</span></span><br><span class="line">scrapy crawl hsw_spider</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写爬虫：打开项目，检查hsw_spider.py的name、allowed_domains、start_urls，编写初步的爬虫.(若运行报错，关闭settings.py的ROBOTSTXT_OBEY)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">HswSpiderSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;hsw_spider&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;www.huibooks.com&quot;</span>]</span><br><span class="line">    start_urls = [<span class="string">&quot;https://www.huibooks.com/wx&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        src_list = response.xpath(<span class="string">&#x27;//li[@class=&quot;post-list-item item-post-style-1&quot;]//img/@src&#x27;</span>)</span><br><span class="line">        name_list = response.xpath(<span class="string">&#x27;//li[@class=&quot;post-list-item item-post-style-1&quot;]//img/@alt&#x27;</span>)</span><br><span class="line">        number_list = response.xpath(<span class="string">&#x27;//li[@class=&quot;post-list-meta-views&quot;]/span/text()&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(src_list)):</span><br><span class="line">            src = src_list[i].extract()</span><br><span class="line">            name = name_list[i].extract()</span><br><span class="line">            number = number_list[i].extract()</span><br><span class="line">            <span class="built_in">print</span>(src,name,number)</span><br></pre></td></tr></table></figure>
</li>
<li><p>定义数据结构：在items.py编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HswItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    src = scrapy.Field()</span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    number = scrapy.Field()</span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>管道文件：在pipelines.py下，处理下载的数据。</p>
<p>在settings.py打开：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&quot;hsw.pipelines.HswPipeline&quot;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注：只能是open_spider和close_spider。写入时只能是字符串对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">HswPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.fp = <span class="built_in">open</span>(<span class="string">&#x27;books.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        self.fp.write(<span class="built_in">str</span>(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spidre</span>):</span><br><span class="line">        self.fp.close()</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入数据结构方面，创建数据结构，存储数据</p>
<p>在hsw_spider.py中输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> hsw.items <span class="keyword">import</span> HswItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HswSpiderSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;hsw_spider&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;www.huibooks.com&quot;</span>]</span><br><span class="line">    start_urls = [<span class="string">&quot;https://www.huibooks.com/wx&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        src_list = response.xpath(<span class="string">&#x27;//li[@class=&quot;post-list-item item-post-style-1&quot;]//img/@src&#x27;</span>)</span><br><span class="line">        name_list = response.xpath(<span class="string">&#x27;//li[@class=&quot;post-list-item item-post-style-1&quot;]//img/@alt&#x27;</span>)</span><br><span class="line">        number_list = response.xpath(<span class="string">&#x27;//li[@class=&quot;post-list-meta-views&quot;]/span/text()&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(src_list)):</span><br><span class="line">            src = src_list[i].extract()</span><br><span class="line">            name = name_list[i].extract()</span><br><span class="line">            number = number_list[i].extract()</span><br><span class="line"></span><br><span class="line">            book = HswItem(src=src, name=name, number=number)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> book</span><br></pre></td></tr></table></figure>
</li>
<li><p>开启多条管道(注意shell运行的路径)</p>
<ul>
<li><p>定义管道类：在pipelines.py下添加类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HswPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.fp = <span class="built_in">open</span>(<span class="string">&#x27;books.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        self.fp.write(<span class="built_in">str</span>(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spidre</span>):</span><br><span class="line">        self.fp.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HswDownloadPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        url = item.get(<span class="string">&quot;src&quot;</span>)</span><br><span class="line">        filename = <span class="string">&#x27;./books/&#x27;</span> + item.get(<span class="string">&quot;name&quot;</span>) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">        </span><br><span class="line">        urllib.request.urlretrieve(url=url,filename=filename)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
</li>
<li><p>在settings.py中开启管道</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&quot;hsw.pipelines.HswPipeline&quot;</span>: <span class="number">300</span>,</span><br><span class="line">   <span class="string">&quot;hsw.pipelines.HswDownloadPipeline&quot;</span>:<span class="number">301</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>多页数据下载</p>
<ul>
<li><p>找到url的规律</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.huibooks.com/wx</span></span><br><span class="line"><span class="comment"># https://www.huibooks.com/wx/page/2</span></span><br><span class="line"><span class="comment"># https://www.huibooks.com/wx/page/3</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>调用hsw_spider.py中的parse函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> hsw.items <span class="keyword">import</span> HswItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HswSpiderSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;hsw_spider&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;www.huibooks.com&quot;</span>]</span><br><span class="line">    start_urls = [<span class="string">&quot;https://www.huibooks.com/wx&quot;</span>]</span><br><span class="line"></span><br><span class="line">    base_url = <span class="string">&#x27;https://www.huibooks.com/wx/page/&#x27;</span></span><br><span class="line">    page = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        src_list = response.xpath(<span class="string">&#x27;//li[@class=&quot;post-list-item item-post-style-1&quot;]//img/@src&#x27;</span>)</span><br><span class="line">        name_list = response.xpath(<span class="string">&#x27;//li[@class=&quot;post-list-item item-post-style-1&quot;]//img/@alt&#x27;</span>)</span><br><span class="line">        number_list = response.xpath(<span class="string">&#x27;//li[@class=&quot;post-list-meta-views&quot;]/span/text()&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(src_list)):</span><br><span class="line">            src = src_list[i].extract()</span><br><span class="line">            name = name_list[i].extract()</span><br><span class="line">            number = number_list[i].extract()</span><br><span class="line"></span><br><span class="line">            book = HswItem(src=src, name=name, number=number)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> book</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># https://www.huibooks.com/wx</span></span><br><span class="line">        <span class="comment"># https://www.huibooks.com/wx/page/2</span></span><br><span class="line">        <span class="comment"># https://www.huibooks.com/wx/page/3</span></span><br><span class="line">        <span class="keyword">if</span> self.page &lt; <span class="number">3</span>:</span><br><span class="line">            self.page += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            url = self.base_url + <span class="built_in">str</span>(self.page)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 调用parse方法</span></span><br><span class="line">            <span class="comment"># scrapy.Request就是 scrapy 的get请求</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url,callback=self.parse) <span class="comment"># callback调用的函数，调用的函数不需要加()</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="3-案例2（多页下载）：电影天堂"><a href="#3-案例2（多页下载）：电影天堂" class="headerlink" title="3.案例2（多页下载）：电影天堂"></a>3.案例2（多页下载）：<a target="_blank" rel="noopener" href="https://www.dyttcn.com/">电影天堂</a></h3><blockquote>
<p>需求：一个item包含多级页面的数据</p>
</blockquote>
<ol>
<li><p>创建项目：切换到项目存放目录，创建项目。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd C:\Users\wuhen\Desktop</span><br><span class="line"></span><br><span class="line">scrapy startproject dytt</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建爬虫文件：进入爬虫存储文件，创建爬虫。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd dytt/dytt/spiders</span><br><span class="line"></span><br><span class="line">scrapy genspider dytt_spider https://www.dyttcn.com/dongzuopian/list_1_1.html</span><br></pre></td></tr></table></figure>
</li>
<li><p>爬虫运行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">运行代码</span></span><br><span class="line">scrapy crawl hsw_spider</span><br></pre></td></tr></table></figure>
</li>
<li><p>定义数据结构：在items.py编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DyttItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    src = scrapy.Field()</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写爬虫：打开项目，检查hsw_spider.py的name、allowed_domains、start_urls，编写初步的爬虫.(注意meta传参)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> dytt.items <span class="keyword">import</span> DyttItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DyttSpiderSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;dytt_spider&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;www.dyttcn.com&quot;</span>]</span><br><span class="line">    start_urls = [<span class="string">&quot;https://www.dyttcn.com/dongzuopian/list_1_1.html&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 要第一页的名字 和 第二页的图片</span></span><br><span class="line">        name_list = response.xpath(<span class="string">&#x27;//table[@class=&quot;tbspan&quot;]//a[@class=&quot;ulink&quot;][2]/text()&#x27;</span>)</span><br><span class="line">        href_list = response.xpath(<span class="string">&#x27;//table[@class=&quot;tbspan&quot;]//a[@class=&quot;ulink&quot;][1]/@href&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(name_list)):</span><br><span class="line">            name = name_list[i].extract()</span><br><span class="line">            href = href_list[i].extract()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 第二页的地址</span></span><br><span class="line">            url = <span class="string">&#x27;https://www.dyttcn.com&#x27;</span> + href</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 访问第二页链接</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url,callback=self.parse_second,meta=&#123;<span class="string">&#x27;name&#x27;</span>:name&#125;) <span class="comment"># meta强行转换成字典</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_second</span>(<span class="params">self,response</span>):</span><br><span class="line">        <span class="comment"># 若没有数据，修改xpath</span></span><br><span class="line">        src = response.xpath(<span class="string">&#x27;//div[@id=&quot;Zoom&quot;]/div[1]/img/@src&#x27;</span>).extract_first()</span><br><span class="line">        name = response.meta[<span class="string">&#x27;name&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        movie = DyttItem(name=name,src=src)</span><br><span class="line">        <span class="keyword">yield</span> movie</span><br></pre></td></tr></table></figure>
</li>
<li><p>管道文件：在pipelines.py下，处理下载的数据。</p>
<p>在settings.py打开：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&quot;dytt.pipelines.DyttPipeline&quot;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注：只能是open_spider和close_spider。写入时只能是字符串对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DyttPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        self.fp = <span class="built_in">open</span>(<span class="string">&#x27;movie.json&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        self.fp.write(<span class="built_in">str</span>(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        self.fp.close()</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="4-CrawlSpider"><a href="#4-CrawlSpider" class="headerlink" title="4.CrawlSpider"></a>4.CrawlSpider</h2><h3 id="1-概念-3"><a href="#1-概念-3" class="headerlink" title="1.概念"></a>1.概念</h3><ol>
<li><p>继承自scrapy.Spider</p>
</li>
<li><p>独门秘笈</p>
<ul>
<li>CrawlSpider可以定义规则，再解析html内容的时候，可以根据链接规则提取出指定的链接，然后再向这些链接发送请求</li>
<li>所以，如果有需要跟进链接的需求，意思就是爬取了网页之后，需要提取链接再次爬取，使用CrawlSpider是非常合适的</li>
</ul>
</li>
<li><p>提取链接</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">链接提取器，在这里就可以写规则提取指定链接</span><br><span class="line">scrapy.linkextractors.LinkExtractor(</span><br><span class="line">    allow = (), # 正则表达式 提取符合正则的链接</span><br><span class="line">    deny = (), 	# (不用)正则表达式 不提取符合正则的链接</span><br><span class="line">    allow_domains = (), # （不用）允许的域名</span><br><span class="line">    deny_domains = (), 	# （不用）不允许的域名</span><br><span class="line">    restrict_xpaths = (), 	# xpath，提取符合xpath规则的链接</span><br><span class="line">    restrict_css = () 		# 提取符合选择器规则的链接</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p>模拟使用</p>
<ul>
<li>正则用法：links1 &#x3D; LinkExtractor(allow&#x3D;r’list_23_\d+.html’)</li>
<li>xpath用法：links2 &#x3D; LinkExtractor(restrict_xpaths&#x3D;r’&#x2F;&#x2F;div[@class&#x3D;”x”]’)</li>
<li>css用法：links3 &#x3D; LinkExtractor(restrict_css&#x3D;’.x’)</li>
</ul>
</li>
<li><p>提取连接</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">link.extract_links(response)</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意事项</p>
<ul>
<li><p>callback只能写函数名字符串, callback&#x3D;’parse_item’</p>
</li>
<li><p>在基本的spider中，如果重新发送请求，那里的callback写的是 callback&#x3D;self.parse_item </p>
<p>**注:**follow&#x3D;true 是否跟进 就是按照提取连接规则进行提取</p>
</li>
</ul>
</li>
</ol>
<h3 id="2-案例：读书网"><a href="#2-案例：读书网" class="headerlink" title="2.案例：读书网"></a>2.案例：<a target="_blank" rel="noopener" href="https://www.dushu.com/">读书网</a></h3><ol>
<li><p>创建项目</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd C:\Users\wuhen\Desktop</span><br><span class="line"></span><br><span class="line">scrapy startproject readbook</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建爬虫文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd readbook/readbook/spiders</span><br><span class="line"></span><br><span class="line">scrapy genspider -t crawl read https://www.dushu.com/book/1188.html</span><br></pre></td></tr></table></figure>
</li>
<li><p>定义数据结构：在items.py编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ReadbookItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    src = scrapy.Field()</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写爬虫：read.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> readbook.items <span class="keyword">import</span> ReadbookItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ReadSpider</span>(<span class="title class_ inherited__">CrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&quot;read&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;www.dushu.com&quot;</span>]</span><br><span class="line">    start_urls = [<span class="string">&quot;https://www.dushu.com/book/1188_1.html&quot;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&quot;/book/1188_\d+\.html&quot;</span>),</span><br><span class="line">             callback=<span class="string">&quot;parse_item&quot;</span>,</span><br><span class="line">             follow=<span class="literal">False</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        img_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;bookslist&quot;]//img&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> img <span class="keyword">in</span> img_list:</span><br><span class="line">            name = img.xpath(<span class="string">&#x27;./@alt&#x27;</span>).extract_first()</span><br><span class="line">            src = img.xpath(<span class="string">&#x27;./@data-original&#x27;</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            book = ReadbookItem(name=name,src=src)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> book</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>管道文件：在settings.py打开管道。在pipelines.py下，处理下载的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ReadbookPipeline</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        self.fp = <span class="built_in">open</span>(<span class="string">&#x27;book.json&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        self.fp.write(<span class="built_in">str</span>(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        self.fp.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>创建数据库与表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database spider01 charser<span class="operator">=</span>utf8;</span><br><span class="line"></span><br><span class="line">use spider01</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> book(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary</span> key auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">128</span>),</span><br><span class="line">    src <span class="type">varchar</span>(<span class="number">128</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>


</li>
<li><p>连接数据库：在settings.py任意位置输入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据库名字</span></span><br><span class="line">DB_HOST = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">DB_PORT = <span class="number">3306</span></span><br><span class="line">DB_USER = <span class="string">&quot;root&quot;</span></span><br><span class="line">DB_PASSWORD = <span class="string">&quot;root&quot;</span></span><br><span class="line">DB_NAME = <span class="string">&quot;spider01&quot;</span></span><br><span class="line">DB_CHARSER = <span class="string">&quot;utf8&quot;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>管道处理数据：在pipelines.py下（注意安装pymysql）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ReadbookPipeline</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        self.fp = <span class="built_in">open</span>(<span class="string">&#x27;book.json&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        self.fp.write(<span class="built_in">str</span>(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        self.fp.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载settings文件</span></span><br><span class="line"><span class="keyword">from</span> scrapy.utils.project <span class="keyword">import</span> get_project_settings</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MysqlPipeline</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        settings = get_project_settings()</span><br><span class="line">        self.host = settings[<span class="string">&#x27;DB_HOST&#x27;</span>]</span><br><span class="line">        self.port = settings[<span class="string">&#x27;DB_PORT&#x27;</span>]</span><br><span class="line">        self.user = settings[<span class="string">&#x27;DB_USER&#x27;</span>]</span><br><span class="line">        self.password =settings[<span class="string">&#x27;DB_PASSWORD&#x27;</span>]</span><br><span class="line">        self.name =settings[<span class="string">&#x27;DB_NAME&#x27;</span>]</span><br><span class="line">        self.charset =settings[<span class="string">&#x27;DB_CHARSER&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        self.connect()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">connect</span>(<span class="params">self</span>):</span><br><span class="line">        self.conn = pymysql.connect(</span><br><span class="line">            host=self.host,</span><br><span class="line">            port=self.port,</span><br><span class="line">            user=self.user,</span><br><span class="line">            password=self.password,</span><br><span class="line">            db=self.name,</span><br><span class="line">            charset=self.charset</span><br><span class="line">        )</span><br><span class="line">        self.cursor = self.conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        sql = <span class="string">&#x27;insert into book(name,src) values(&quot;&#123;&#125;&quot;,&quot;&#123;&#125;&quot;)&#x27;</span>.<span class="built_in">format</span>(item[<span class="string">&#x27;name&#x27;</span>],item[<span class="string">&#x27;src&#x27;</span>])</span><br><span class="line">        self.cursor.execute(sql)</span><br><span class="line">        self.conn.commit()</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.cursor.close()</span><br><span class="line">        self.conn.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="5-日志信息与日志级别"><a href="#5-日志信息与日志级别" class="headerlink" title="5.日志信息与日志级别"></a>5.日志信息与日志级别</h2><ol>
<li><p>日志级别：</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CRITICAL：	严重错误</span><br><span class="line">ERROR： 		一般错误</span><br><span class="line">WARNING： 	警告</span><br><span class="line">INFO: 		一般信息</span><br><span class="line">DEBUG： 		调试信息</span><br><span class="line"></span><br><span class="line">1.从下到上级别越高</span><br><span class="line">2.默认的日志等级是DEBUG,只要出现了DEBUG或者DEBUG以上等级的日志那么这些日志将会打印</span><br></pre></td></tr></table></figure>
</li>
<li><p>settings.py文件设置：</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LOG_FILE : 将屏幕显示的信息全部记录到文件中，屏幕不再显示，注意文件后缀一定是.log</span><br><span class="line">    LOG_FILE=&#x27;logdemo.log&#x27;</span><br><span class="line"></span><br><span class="line">LOG_LEVEL : 设置日志显示的等级，就是显示哪些，不显示哪些</span><br><span class="line">	LOG_LEVEL=&#x27;WARNING&#x27;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="6-scrapy的post请求"><a href="#6-scrapy的post请求" class="headerlink" title="6.scrapy的post请求"></a>6.scrapy的post请求</h2><h3 id="1-方法"><a href="#1-方法" class="headerlink" title="1.方法"></a>1.方法</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">（1）重写start_requests方法：</span><br><span class="line">    def start_requests(self)</span><br><span class="line">(2) start_requests的返回值：</span><br><span class="line">    scrapy.FormRequest(url=url, headers=headers, callback=self.parse_item, formdata=data)</span><br><span class="line">        url: 要发送的post地址</span><br><span class="line">        headers：可以定制头信息</span><br><span class="line">        callback: 回调函数</span><br><span class="line">        formdata: post所携带的数据，这是一个字典</span><br></pre></td></tr></table></figure>



<h3 id="2-案例：百度翻译"><a href="#2-案例：百度翻译" class="headerlink" title="2.案例：百度翻译"></a>2.案例：百度翻译</h3><ol>
<li><p>创建项目</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd C:\Users\wuhen\Desktop</span><br><span class="line"></span><br><span class="line">scrapy startproject post_spider</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建爬虫</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd post_spider/post_spider/spiders</span><br><span class="line"></span><br><span class="line">scrapy genspider postDemo https://fanyi.baidu.com/sug</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写爬虫：post请求没有参数，则请求无意义，所以start_urls无用，parse方法无用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PostdemoSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;postDemo&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;fanyi.baidu.com&quot;</span>]</span><br><span class="line">    <span class="comment"># post请求没有参数，则请求无意义，所以start_urls无用，parse方法无用</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        url = <span class="string">&#x27;https://fanyi.baidu.com/sug&#x27;</span></span><br><span class="line"></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&#x27;kw&#x27;</span>:<span class="string">&#x27;final&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> scrapy.FormRequest(url=url,formdata=data,callback=self.parse_second)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_second</span>(<span class="params">self,response</span>):</span><br><span class="line">        content = response.text</span><br><span class="line">        obj = json.loads(content)</span><br><span class="line">        <span class="built_in">print</span>(obj)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="7-代理"><a href="#7-代理" class="headerlink" title="7.代理"></a>7.代理</h2><ol>
<li><p>到settings.py中，打开一个选项</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">&#x27;postproject.middlewares.Proxy&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>到middlewares.py中写代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">    request.meta[<span class="string">&#x27;proxy&#x27;</span>] = <span class="string">&#x27;https://113.68.202.10:9999&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure></li>
</ol>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a></div><div class="post_share"><div class="social-share" data-image="/img/%E5%9B%BE%E7%89%87/014.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/11/01/python%E5%9F%BA%E7%A1%80/" title="Python基础"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/%E5%9B%BE%E7%89%87/011.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Python基础</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/11/01/python%E5%9F%BA%E7%A1%80/" title="Python基础"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/%E5%9B%BE%E7%89%87/011.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-01</div><div class="title">Python基础</div></div></a></div><div><a href="/2023/11/20/python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/" title="python数据分析与可视化"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/%E5%9B%BE%E7%89%87/013.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-20</div><div class="title">python数据分析与可视化</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/touxiang.JPG" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HQH</div><div class="author-info__description">我要睡到自然醒</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/silent-wuhen"><i class="fab fa-github"></i><span>GitHub</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">此网站仅个人学习使用，如有侵权请联系QQ：3281320387删除。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E5%89%8D%E6%8F%90%E7%9F%A5%E8%AF%86"><span class="toc-number">2.</span> <span class="toc-text">1.前提知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-number">2.1.</span> <span class="toc-text">1.序列化与反序列化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A6%82%E5%BF%B5"><span class="toc-number">2.1.1.</span> <span class="toc-text">1.概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%BA%8F%E5%88%97%E5%8C%96%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.1.2.</span> <span class="toc-text">2.序列化的实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.1.3.</span> <span class="toc-text">3.反序列化的实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%BC%82%E5%B8%B8"><span class="toc-number">2.2.</span> <span class="toc-text">2.异常</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A6%82%E5%BF%B5-1"><span class="toc-number">2.2.1.</span> <span class="toc-text">1.概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-try%E2%80%A6except%E8%AF%AD%E5%8F%A5"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.try…except语句</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%BD%91%E9%A1%B5%E9%A1%B5%E9%9D%A2%E7%BB%93%E6%9E%84%E7%9A%84%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.3.</span> <span class="toc-text">3.网页页面结构的介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-html%E6%96%87%E4%BB%B6"><span class="toc-number">2.3.1.</span> <span class="toc-text">1.html文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-html%E7%9A%84%E6%A0%87%E7%AD%BE%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.3.2.</span> <span class="toc-text">2.html的标签介绍</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-URL%EF%BC%88%E7%BB%9F%E4%B8%80%E8%B5%84%E6%BA%90%E5%AE%9A%E4%BD%8D%E7%AC%A6%EF%BC%89%E7%BB%84%E6%88%90"><span class="toc-number">2.4.</span> <span class="toc-text">4.URL（统一资源定位符）组成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%92%E8%81%94%E7%BD%91%E7%88%AC%E8%99%AB"><span class="toc-number">2.5.</span> <span class="toc-text">5.什么是互联网爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E7%88%AC%E8%99%AB%E6%A0%B8%E5%BF%83"><span class="toc-number">2.6.</span> <span class="toc-text">6.爬虫核心</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E7%88%AC%E8%99%AB%E7%9A%84%E7%94%A8%E9%80%94"><span class="toc-number">2.7.</span> <span class="toc-text">7.爬虫的用途</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E7%88%AC%E8%99%AB%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-number">2.8.</span> <span class="toc-text">8.爬虫的分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB-%E4%B8%8D%E6%98%AF%E6%88%91%E4%BB%AC%E5%AD%A6%E4%B9%A0%E7%9A%84"><span class="toc-number">2.8.1.</span> <span class="toc-text">1.通用爬虫(不是我们学习的)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%81%9A%E7%84%A6%E7%88%AC%E8%99%AB"><span class="toc-number">2.8.2.</span> <span class="toc-text">2.聚焦爬虫</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E5%8F%8D%E7%88%AC%E8%99%AB%E6%89%8B%E6%AE%B5%EF%BC%9F"><span class="toc-number">2.9.</span> <span class="toc-text">9.反爬虫手段？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Urllib"><span class="toc-number">3.</span> <span class="toc-text">2.Urllib</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-urllib%E5%BA%93%E4%BD%BF%E7%94%A8"><span class="toc-number">3.1.</span> <span class="toc-text">1.urllib库使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-urllib%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">3.1.1.</span> <span class="toc-text">1.urllib的基本使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Urllib%E7%9A%841%E4%B8%AA%E7%B1%BB%E5%9E%8B%E5%92%8C6%E4%B8%AA%E6%96%B9%E6%B3%95"><span class="toc-number">3.1.2.</span> <span class="toc-text">2.Urllib的1个类型和6个方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Urllib%E7%9A%84%E4%B8%8B%E8%BD%BD"><span class="toc-number">3.1.3.</span> <span class="toc-text">3.Urllib的下载</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E8%AF%B7%E6%B1%82%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%AE%9A%E5%88%B6"><span class="toc-number">3.2.</span> <span class="toc-text">2.请求对象的定制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-UA%E4%BB%8B%E7%BB%8D%EF%BC%9A"><span class="toc-number">3.2.1.</span> <span class="toc-text">1.UA介绍：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-UA%E5%8F%8D%E7%88%AC"><span class="toc-number">3.2.2.</span> <span class="toc-text">2.UA反爬</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%89%A9%E5%B1%95%EF%BC%9A%E7%BC%96%E7%A0%81"><span class="toc-number">3.2.3.</span> <span class="toc-text">3.扩展：编码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%BC%96-%E8%A7%A3%E7%A0%81"><span class="toc-number">3.3.</span> <span class="toc-text">3.编&#x2F;解码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-get%E8%AF%B7%E6%B1%82%E6%96%B9%E5%BC%8F%EF%BC%9Aurllib-parse-quote%EF%BC%88%EF%BC%89"><span class="toc-number">3.3.1.</span> <span class="toc-text">1.get请求方式：urllib.parse.quote（）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-get%E8%AF%B7%E6%B1%82%E6%96%B9%E5%BC%8F%EF%BC%9Aurllib-parse-urlencode%EF%BC%88%EF%BC%89"><span class="toc-number">3.3.2.</span> <span class="toc-text">2.get请求方式：urllib.parse.urlencode（）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-post%E8%AF%B7%E6%B1%82%E6%96%B9%E5%BC%8F"><span class="toc-number">3.3.3.</span> <span class="toc-text">3.post请求方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%80%BB%E7%BB%93%EF%BC%9Apost%E5%92%8Cget%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">3.3.4.</span> <span class="toc-text">4.总结：post和get区别？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%91%E4%B9%8B%E8%AF%A6%E7%BB%86%E7%BF%BB%E8%AF%91"><span class="toc-number">3.3.5.</span> <span class="toc-text">5.百度翻译之详细翻译</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-ajax%E7%9A%84get%E8%AF%B7%E6%B1%82"><span class="toc-number">3.4.</span> <span class="toc-text">4.ajax的get请求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-ajax%E7%9A%84post%E8%AF%B7%E6%B1%82"><span class="toc-number">3.5.</span> <span class="toc-text">5.ajax的post请求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-URLError-HTTPError"><span class="toc-number">3.6.</span> <span class="toc-text">6.URLError\HTTPError</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-cookie%E7%99%BB%E5%BD%95"><span class="toc-number">3.7.</span> <span class="toc-text">7.cookie登录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Handler%E5%A4%84%E7%90%86%E5%99%A8"><span class="toc-number">3.8.</span> <span class="toc-text">8.Handler处理器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-number">3.9.</span> <span class="toc-text">9.代理服务器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E8%A7%A3%E6%9E%90"><span class="toc-number">4.</span> <span class="toc-text">3.解析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-xpath"><span class="toc-number">4.1.</span> <span class="toc-text">1.xpath</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-JsonPath"><span class="toc-number">4.2.</span> <span class="toc-text">2.JsonPath</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-BeautifulSoup"><span class="toc-number">4.3.</span> <span class="toc-text">3.BeautifulSoup</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-selenium"><span class="toc-number">5.</span> <span class="toc-text">4.selenium</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-selenium%E6%A6%82%E8%BF%B0"><span class="toc-number">5.1.</span> <span class="toc-text">1.selenium概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Chrome-handless"><span class="toc-number">5.2.</span> <span class="toc-text">2.Chrome handless</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-requests"><span class="toc-number">6.</span> <span class="toc-text">5.requests</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">6.1.</span> <span class="toc-text">1.基本使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-get%E8%AF%B7%E6%B1%82"><span class="toc-number">6.2.</span> <span class="toc-text">2.get请求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-post%E8%AF%B7%E6%B1%82"><span class="toc-number">6.3.</span> <span class="toc-text">3.post请求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E4%BB%A3%E7%90%86"><span class="toc-number">6.4.</span> <span class="toc-text">4.代理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-cookie%E5%AE%9A%E5%88%B6"><span class="toc-number">6.5.</span> <span class="toc-text">5.cookie定制</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-scrapy"><span class="toc-number">7.</span> <span class="toc-text">6.scrapy</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-scrapy%E6%A6%82%E8%BF%B0"><span class="toc-number">7.1.</span> <span class="toc-text">1.scrapy概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%AE%A4%E8%AF%86scrapy"><span class="toc-number">7.1.1.</span> <span class="toc-text">1.认识scrapy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-scrapy%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%88%9B%E5%BB%BA%E4%BB%A5%E5%8F%8A%E8%BF%90%E8%A1%8C"><span class="toc-number">7.1.2.</span> <span class="toc-text">2.scrapy项目的创建以及运行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-scrapy%E6%9E%B6%E6%9E%84%E7%BB%84%E6%88%90"><span class="toc-number">7.1.3.</span> <span class="toc-text">3.scrapy架构组成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-scrapy%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">7.1.4.</span> <span class="toc-text">4.scrapy工作原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-scrapy-shell"><span class="toc-number">7.2.</span> <span class="toc-text">2.scrapy shell</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-yield"><span class="toc-number">7.3.</span> <span class="toc-text">3.yield</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A6%82%E5%BF%B5-2"><span class="toc-number">7.3.1.</span> <span class="toc-text">1.概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A1%88%E4%BE%8B1%EF%BC%88%E6%8C%89%E9%A1%B5%E4%B8%8B%E8%BD%BD%EF%BC%89%EF%BC%9A%E7%88%AC%E5%8F%96%E6%B1%87%E4%B9%A6%E7%BD%91"><span class="toc-number">7.3.2.</span> <span class="toc-text">2.案例1（按页下载）：爬取汇书网</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%A1%88%E4%BE%8B2%EF%BC%88%E5%A4%9A%E9%A1%B5%E4%B8%8B%E8%BD%BD%EF%BC%89%EF%BC%9A%E7%94%B5%E5%BD%B1%E5%A4%A9%E5%A0%82"><span class="toc-number">7.3.3.</span> <span class="toc-text">3.案例2（多页下载）：电影天堂</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-CrawlSpider"><span class="toc-number">7.4.</span> <span class="toc-text">4.CrawlSpider</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A6%82%E5%BF%B5-3"><span class="toc-number">7.4.1.</span> <span class="toc-text">1.概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A1%88%E4%BE%8B%EF%BC%9A%E8%AF%BB%E4%B9%A6%E7%BD%91"><span class="toc-number">7.4.2.</span> <span class="toc-text">2.案例：读书网</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%97%A5%E5%BF%97%E4%BF%A1%E6%81%AF%E4%B8%8E%E6%97%A5%E5%BF%97%E7%BA%A7%E5%88%AB"><span class="toc-number">7.5.</span> <span class="toc-text">5.日志信息与日志级别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-scrapy%E7%9A%84post%E8%AF%B7%E6%B1%82"><span class="toc-number">7.6.</span> <span class="toc-text">6.scrapy的post请求</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%96%B9%E6%B3%95"><span class="toc-number">7.6.1.</span> <span class="toc-text">1.方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A1%88%E4%BE%8B%EF%BC%9A%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%91"><span class="toc-number">7.6.2.</span> <span class="toc-text">2.案例：百度翻译</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E4%BB%A3%E7%90%86"><span class="toc-number">7.7.</span> <span class="toc-text">7.代理</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/04/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E9%80%9F%E6%88%90/" title="计算机科学速成"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/%E5%9B%BE%E7%89%87/015.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机科学速成"/></a><div class="content"><a class="title" href="/2025/04/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E9%80%9F%E6%88%90/" title="计算机科学速成">计算机科学速成</a><time datetime="2025-04-22T03:53:42.000Z" title="发表于 2025-04-22 11:53:42">2025-04-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/09/12/Pytorch%E5%85%A5%E9%97%A8_%E5%B0%8F%E5%9C%9F%E5%A0%86/" title="pytorch入门_小土堆"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/%E5%9B%BE%E7%89%87/016.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="pytorch入门_小土堆"/></a><div class="content"><a class="title" href="/2024/09/12/Pytorch%E5%85%A5%E9%97%A8_%E5%B0%8F%E5%9C%9F%E5%A0%86/" title="pytorch入门_小土堆">pytorch入门_小土堆</a><time datetime="2024-09-12T15:17:12.000Z" title="发表于 2024-09-12 23:17:12">2024-09-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/09/12/%E5%95%86%E5%93%81%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E7%B3%BB%E7%BB%9F/" title="商品大数据可视化系统"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/%E5%9B%BE%E7%89%87/011.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="商品大数据可视化系统"/></a><div class="content"><a class="title" href="/2024/09/12/%E5%95%86%E5%93%81%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E7%B3%BB%E7%BB%9F/" title="商品大数据可视化系统">商品大数据可视化系统</a><time datetime="2024-09-12T15:17:12.000Z" title="发表于 2024-09-12 23:17:12">2024-09-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/04/28/Git%E5%9F%BA%E7%A1%80/" title="Git基础"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/%E5%9B%BE%E7%89%87/016.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Git基础"/></a><div class="content"><a class="title" href="/2024/04/28/Git%E5%9F%BA%E7%A1%80/" title="Git基础">Git基础</a><time datetime="2024-04-28T11:00:12.000Z" title="发表于 2024-04-28 19:00:12">2024-04-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/04/28/Shell%E7%BC%96%E7%A8%8B/" title="Shell编程"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/%E5%9B%BE%E7%89%87/012.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Shell编程"/></a><div class="content"><a class="title" href="/2024/04/28/Shell%E7%BC%96%E7%A8%8B/" title="Shell编程">Shell编程</a><time datetime="2024-04-28T11:00:12.000Z" title="发表于 2024-04-28 19:00:12">2024-04-28</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By HQH</div><div class="footer_custom_text">Hi,welcome to my blog!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><script defer src="/js/cursor.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="30" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="false" data-text="富强,民主,文明,和谐" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v6.2.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.1" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用多线部署，主线路托管于Vercel" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://dashboard.4everland.org/" style="margin-inline:5px" data-title="本站采用多线部署，备用线路托管于4EVERLAND" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-4EVERLAND-22DDDD?style=flat&amp;logo=IPFS" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '1s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><!-- hexo injector body_end end --></body></html>